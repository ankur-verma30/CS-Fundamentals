ğŸ§  Consistent Hashing

ğŸ”¹ 1. Problem Definition
âŒ Whatâ€™s Not the Problem:
Not load balancing in the traditional sense.

âœ… Real Problem:
When adding/removing servers, a naive hash function remaps almost all keys to new servers â†’ destroys cache locality and stickiness.

ğŸ”¹ 2. Naive Approach Fails
â¤ Using hash(request_id) % N:
â¤ Adding/removing a server changes N.
â¤ Causes complete remapping of all request-server mappings.
â¤ Results in cache misses and high latency.

â— Very inefficient for scalable systems.

ğŸ”¹ 3. Consistent Hashing: The Ring Model
ğŸ“Œ Core Concept:
â¤ Imagine the hash space [0, M-1] laid out as a circle.
â¤ Both requests and servers are hashed to positions on the ring.

ğŸ” Mapping Rules:
â¤ Hash(request_id) â†’ maps to a point on the ring.
â¤ Hash(server_id) â†’ server also maps to a point.
â¤ A request is routed clockwise to the next server on the ring.

4. Example Walkthrough
â¤ Letâ€™s say M = 30.
â¤ Servers: S1, S2, S3, S4 â†’ hashed to ring positions like 19, 5, 23, 28.
â¤ Request hash(id) = 7 â†’ goes to first server clockwise (e.g., S2 at 19).
â¤ Requests are evenly distributed assuming uniform hashing.

ğŸ”¹ 5. Why It Works
ğŸ¯ Load Distribution:
â¤ Because hashing is uniform, servers get approximately equal loads.
â¤ Each server gets requests from the arc before its point on the ring.

ğŸ”„ On Adding Server:
â¤ Only requests that now land before the new serverâ€™s position shift to it.
â¤ Minimal movement of keys.

âŒ On Removing Server:
â¤ Its requests are rerouted to the next server clockwise.
â¤ Again, only a small subset of keys remapped.

ğŸ”¹ 6. Real Problem: Uneven Distribution
â¤ Even with uniform hashing:
â¤ If you only have a few servers, distribution might be skewed.

Example:
S1 ends up handling 50% of requests due to unlucky hash positioning.

ğŸ”¹ 7. Solution: Virtual Nodes
ğŸ’¡ Concept:
Instead of one point per server, create multiple virtual nodes per server.

âš™ï¸ How:
â¤ Hash the same server ID multiple times using K different hash functions:
â¤ H1(S1), H2(S1), ..., Hk(S1) â†’ server appears at K points on the ring.

ğŸ” Benefits:
â¤ Smooths out load imbalances.
â¤ Adds redundancy.
â¤ With log(N) virtual nodes per server:
â¤ Expected load distribution is nearly uniform.

ğŸ”¹ 8. On Server Addition or Failure
ğŸŸ¢ Add Server:
â¤ Add K virtual nodes.
â¤ Only the arcs between those nodes and their previous clockwise neighbors are affected.

ğŸ”´ Remove Server:
â¤ Remove K virtual nodes.
â¤ Requests served by them get redistributed evenly to other nodes.

ğŸ”¹ 9. Key Takeaways
| Feature                 | Description                                         |
| ----------------------- | --------------------------------------------------- |
| Stickiness          | Request consistently goes to the same server.       |
| Minimal Rebalancing | Only a few keys remapped on server change.          |
| Cache Locality      | Preserved due to sticky mapping.                    |
| Load Balancing      | Uniform with enough virtual nodes.                  |
| Fault Tolerance     | Easy recovery by redistributing only affected arcs. |

ğŸ”¹ 10. Applications of Consistent Hashing
â¤ ğŸ”¹ Distributed Caches: Redis Cluster, Memcached
â¤ ğŸ”¹ Databases: DynamoDB, Cassandra
â¤ ğŸ”¹ CDNs: Efficient edge request routing
â¤ ğŸ”¹ Sharded Systems: Microservices with session stickiness
â¤ ğŸ”¹ Load Balancers: NGINX or custom balancers in large infra