ğŸ”¹ 1. What is Consistent Hashing?
ğŸ”‘ A load balancing technique used in distributed systems to minimize data re-distribution when servers are added or removed.

â¤ It maps both servers and data(requests) onto a circular hash ring.
â¤ Used in distributed caches(like Redis, Memcached), database (like Cassandra), and load balancers.

ğŸ§© Problem:
â¤ You have a working algorithm hosted on one computers(server).
â¤ As traffic increases, you add more servers.
â¤ You need to distribute user requests across these multiple servers.

â— Challenge:
If you use basic hashing (e.g. hash(key) %N):
â¤ Adding/removing a server changes N, which:
    â¤Causes cache misses, destroys request stickiness, and increase latency.

ğŸ”¹ 3. Naive Hashing Example
Let's Say:
You have 4 servers -> hash(request_id) % 4
Works well, evenly distributes requests

But when a 5th server is added:
â¤ You now do hash(request_id) % 5.
â¤ Result: All previous assignments break
â¤ 100% of key may remap -> destroying cache locality.

ğŸ”¹ 4. Key Terms
| Term              | Meaning                                                     |
| ----------------- | ----------------------------------------------------------- |
| **Request ID**    | Unique identifier (can be user ID or session ID)            |
| **Hash Function** | Converts request ID into a number in range `0 to M-1`       |
| **Bucket/Server** | Target destination that handles the request                 |
| **Stickiness**    | The property of a user consistently hitting the same server |

ğŸ”¹ 5. Why This Matters
â¤ Caches are server-local. If user A always gets server S3.
    â¤ You can store their profile in server S3's local cache.
    â¤ If server assignments change, cache is useless -> leads to slow reads.
â¤ Changing the modulo N breaks request-server mapping consistency.
â¤ â— Goal:Minimize remapping when servers are added/removed.

ğŸ”¹ 6. Consistent Hashing: Core Idea
ğŸ” Hash both servers and keys oto a circular hash ring (0 to MAX).

ğŸ”Steps:
â¤ Hash server names (like "S1", "S2") -> place them on the ring.
â¤ Hash the request ID (key) -> find its position on the ring.
â¤ Move clockwise to find the first server with a hash >= key hash.
â¤ That server handles the reqeust.

ğŸ§  Key Insight:
â¤ Only a small fraction of keys get remapped when a server is added or removed.
â— Compared to 100% in mod N strategy.

ğŸ”¹ 7. Pie Chart Illustration (Visualizing Hash Ring)
â¤ Consider hash space as a pie(circle) of numbers.
â¤ Servers divide the pie into ranges.
â¤ Each server is reponsible for a slice.
â¤ When adding a new server:
    â¤ Take a little load from each server (not from one single server).
    â¤ Total redistribution is minimal and evenly spread.

ğŸ”¹ 8. Benefits of Consistent Hashing
| Benefit                   | Why It Matters                                             |
| ------------------------- | ---------------------------------------------------------- |
| ğŸ”„ Minimal remapping      | Only affects a small subset of keys                        |
| ğŸ’¾ Better cache hit ratio | Stickiness ensures user hits same server                   |
| âš–ï¸ Load balancing         | Uniform distribution of requests                           |
| âš¡ High availability       | Easy to add/remove servers on the fly                      |
| ğŸ§© Scalability            | Works well for distributed databases, CDNs, load balancers |

ğŸ”¹ 9. Virtual Nodes (Advanced Optimization)
â¤ Each physical server is represented by multiple virtual nodes on the ring.
â¤ Helps handle uneven server distribution.
â¤ Ensures better load distribution if some servers are more powerful.

Example: Server S1 may have 3 virtual positions on the ring; S2 may have 5.

ğŸ”¹ 10. Real-World Applications
ğŸ”¹ Distributed Caches: Memcached, Redis Cluster
ğŸ”¹ Distributed Databases: Cassandra, DynamoDB
ğŸ”¹ Load Balancers: NGINX, Envoy
ğŸ”¹ CDNs: Cache routing based on user ID/location