#########################
ğŸ“Œ Tinder System Design
#########################

ğŸ” Prerequisites & Mindset Before System Design
â¤ System design requires strong fundamentals (databases, networking, scalability, storage, consistency, etc.)
â¤ Diagrams wonâ€™t make sense without conceptual understanding
â¤ Each concept discussed can be deep-dived independently
â¤ Interview expectation: clarity of thinking > fancy components
â¤ Ensure basics are clear before jumping into architecture design


ğŸ” Designing Tinder
âŒ Common mistake: candidates jump directly to tech stack (DBs, services, tools)
âœ… Recommended approach:
âœ”ï¸ Step back
âœ”ï¸ Think logically and calmly
âœ”ï¸ Focus on what users want first, not infrastructure


ğŸ”„ Two Possible Approaches to Design the System

1ï¸âƒ£ Data-First (ER Diagram Approach) âŒ
â¤ Start with entities and relationships
â¤ Then design services
â¤ Then design clients
â¤ Problems:
  âœ”ï¸ Too constrained
  âœ”ï¸ Too abstract
  âœ”ï¸ Ignores actual user needs initially

2ï¸âƒ£ Feature-First (Front-to-Back) âœ… (Recommended)
â¤ Start with **user-facing features**
â¤ Break features into **services**
â¤ Define **data requirements per service**

â¤ Advantages:
  âœ”ï¸ Flexible architecture
  âœ”ï¸ Easier to evolve system
  âœ”ï¸ Aligns with real product development
  âœ”ï¸ Interviewer-friendly approach


ğŸ” Feature Selection Strategy
â¤ Do NOT overload with features
â¤ Interview duration â‰ˆ 1 hour max
â¤ Start with 4â€“5 core features only
â¤ Depth > breadth


ğŸ” Core Features Identified for Tinder
1ï¸âƒ£ Storing User Profiles
â¤ Obvious but **confirm with interviewer**
â¤ Profiles include:
  âœ”ï¸ Basic user info
  âœ”ï¸ Images (very important)
  âœ”ï¸ Location

ğŸŒŸ Follow-up Questions
â“How many images per user?
  ğŸ‘‰ Assumption taken: 5 images per user


2ï¸âƒ£ Match Recommendation System
â¤ Users swipe based on preferences
â¤ System recommends profiles

ğŸŒŸ Follow-up Questions to Ask
â¤ Number of active users
â¤ Avoid over-questioning
â¤ Keep assumptions high-level and simple


3ï¸âƒ£ Matching Logic
When two users like each other â†’ match created
ğŸŸ¢ Assumptions
â¤ Match rate assumption (example â€“ India):
  âœ”ï¸ 0.1% chance per swipe
â¤ Estimated matches per user:
  âœ”ï¸ Active Users Ã— 10â»Â³ per day
â¤ Assumptions are acceptable in interviews if stated clearly


4ï¸âƒ£ Direct Messaging (After Match)
Chat allowed only after match
Messaging system is a separate feature
Detailed questions deferred for later design stages


ğŸ” Designing Image Storage 
â“ Core Question
â¤ How should images be stored?
  ğŸ”— File system
  ğŸ”— Database (BLOB â€“ Binary Large Object)  â†’  Used for storing large binary data (images, videos)


ğŸ” Database ğŸ†š File System for Image Storage
ğŸ”„ Arguments FOR Database Storage (BLOBs)

Databases provide:
â¤ Mutability
â¤ Transaction guarantees
â¤ Indexing
â¤ Access control
ğŸ‘‰ Why These Donâ€™t Matter for Images

âŒ Mutability
â¤ Images are rarely edited
â¤ Updates replace the entire image, not partial bits
â¤ Better to treat images as immutable

âŒ Transaction Guarantees
â¤ Images donâ€™t need atomic multi-step operations
â¤ No transactional dependency

âŒ Indexes
â¤ Indexing useful for searchable fields (name, age)
â¤ Binary data = meaningless for indexing

âš ï¸ Access Control
â¤ Valid concern
â¤ But file systems can also enforce access control
â¤ Setup complexity â‰ˆ secure database setup


ğŸŸ¢ Why File Storage is Preferred
â¤ Cheaper than database storage
â¤ Designed specifically for file handling
â¤ Better suited for large static objects (images)
â¤ Scales well with CDN + object storage


ğŸ” Vertical Partitioning vs File Storage (Extended Argument)
â¤ Databases can store large objects separately using vertical partitioning
  âœ”ï¸ Profile table: profile_id
  âœ”ï¸ Image table: image_id â†’ image stored elsewhere
â¤ But if image is already stored separately:
  âœ”ï¸ Why not use a file system directly?


â— Problems with DB-based Image Storage
â¤ Higher cost compared to file storage
â¤ Risk of accidental SELECT * queries pulling heavy data
â¤ In real-world systems, SELECT * is commonly (and mistakenly) used
â¤ File systems naturally avoid this issue


ğŸ” Static Nature of Images
â¤ Profile images are static content
â¤ Ideal candidates for:
    âœ”ï¸ CDN (Content Delivery Network)

â¤ CDN Benefits
    âœ”ï¸ Faster access to images
    âœ”ï¸ Reduced latency
    âœ”ï¸ Offloads traffic from core services

â¤ Database Still Stores Image References
â¤ Database does NOT store the image itself
â¤ It stores:
  âœ”ï¸ image_id
  âœ”ï¸ image_url
  âœ”ï¸ profile_id (owner)


ğŸ”„ Storage Flow
â¤ Image stored in:
  âœ”ï¸ Distributed File System
â¤ Database stores:
  âœ”ï¸ Mapping of profile_id â†’ image_id â†’ image_url


ğŸ” High-Level System Design Begins
â¡ Client Application
â¤ Mobile client sends requests
â¤ All actions initiated by user interaction


ğŸ” Profile Service Responsibilities
â¤ User registration
  âœ”ï¸ username
  âœ”ï¸ password
â¤ Stores user data in database
â¤ Handles authentication mechanisms
  âœ”ï¸ Email verification
  âœ”ï¸ Two-factor authentication (assumed)


ğŸ” Profile Service â€“ Database Schema Updating Profile Flow
â¤ User updates:
  âœ”ï¸ Profile details
  âœ”ï¸ Images


ğŸ” Authentication Approaches
âŒ Username + Password per Request
â¤ Insecure
â¤ Poor practice

âœ… Token-Based Authentication
â¤ Client sends token with request
â¤ Profile service validates token
â¤ Interview-safe keyword: token-based auth


â—Problem with Direct Authentication in Every Service
â¤ Each new service needs auth logic
â¤ Causes:
    âœ”ï¸ Code duplication    
    âœ”ï¸ Tight coupling  
    âœ”ï¸ Poor scalability


ğŸ” Gateway Service 
â¡ Role of Gateway: 
â¤ Single entry point for clients
â¤ Clients NEVER talk directly to services

ğŸ”„ Gateway Responsibilities
â¤ Receives request
â¤ Sends token + username to profile service
â¤ Gets auth decision (yes / no)
â¤ Routes request to correct service
â¤ Forwards response back to client

ğŸ”„ Benefits of Gateway Pattern
â¤ Decouples services
â¤ Centralized authentication
â¤ Prevents duplication of auth logic
â¤ Enables protocol separation (important for chat later)


ğŸ” Image Service (Separate from Profile Service)
â“ Why Separate Image Service?
â¤ Images are heavy
â¤ Other services may need only images:
  âœ”ï¸ ML pipelines
  âœ”ï¸ Analytics
â¤ Profile service often needs only metadata


ğŸ” Image Service Architecture
â¤ Distributed File System
  âœ”ï¸ Stores actual image files
â¤ Database stores references:
  âœ”ï¸ profile_id
  âœ”ï¸ image_id
  âœ”ï¸ image_url


ğŸ” Direct Messaging
â¤ Client A wants to message Client B
â¤ Happens only after match

âœ… Request Example
Send message to user_id_1 from user_id_2


ğŸ” Communication Protocol Primer
ğŸ”„ HTTP (HyperText Transfer Protocol)
â¤ Client â†’ Server â†’ Response
â¤ Server never initiates request
â¤ Not ideal for real-time chat


ğŸ” Chat Protocol Limitation with Clientâ€“Server Model
â¤ Traditional clientâ€“server (HTTP) is **pull-based**
â¤ Client must repeatedly ask:
  âœ”ï¸ Any new messages?

â— Problems:
    âœ”ï¸ Inefficient
    âœ”ï¸ High latency
    âœ”ï¸ Wasted resources


ğŸ” Why Chat Needs Push-Based Communication
â¤ Messages should be pushed to client instantly
â¤ Polling every few seconds is not scalable


ğŸ” Peer-to-Peer Style Communication for Chat
â¤ All machines are treated as equals
â¤ Server can push messages to client
â¤ Better suited for real-time messaging


ğŸ” Protocols for Chat
1ï¸âƒ£ HTTP (Clientâ€“Server)
âœ”ï¸ Request â†’ Response
âœ”ï¸ Server cannot initiate communication
âœ”ï¸ Not ideal for chat

2ï¸âƒ£ XMPP (Preferred Example)
âœ”ï¸ Supports real-time messaging
âœ”ï¸ Server can push messages to client
âœ”ï¸ Important interview keyword


ğŸ” Underlying Connection Mechanism
1ï¸âƒ£ WebSockets
âœ”ï¸ Persistent connection
âœ”ï¸ Enables real-time communication

2ï¸âƒ£ Alternative: TCP
âœ”ï¸ Custom protocol over TCP
âœ”ï¸ Connection maintained
âœ”ï¸ Interview-safe simplification


ğŸ” Managing Active Connections
â— Core Problem
âœ”ï¸ For every connection ID:
    Need to know which user owns it


ğŸ” Session Service (Decoupling Responsibility)
â¤ Gateway should NOT manage connections
â¤ Introduce a separate Session Service

ğŸ” Session Service Responsibilities
â¤ Map: user_id â†’ connection_id
â¤ Track active connections
â¤ Enable message routing


ğŸ” Direct Messaging Flow
1ï¸âƒ£ User A sends message request
2ï¸âƒ£ Match validation needed
3ï¸âƒ£ Session service finds connection for User B
4ï¸âƒ£ Message pushed to correct socket


ğŸ” Noting Matches (Requirement 3)
â“ How to Store Matches on Client-Side Storage?
â¤ Possible
â¤ But server should be source of truth


ğŸ” Matcher Service
â¤ Dedicated service to store matches
â¤ Table structure:
  âœ”ï¸ user_id â†’ matched_user_id

ğŸ”„ Data Duplication
â¤ A â†” B stored twice
â¤ Enables fast lookup

ğŸ”„ Indexing
â¤ Index on user_id
â¤ Efficient match queries


ğŸ” Authorization Before Chat
â¤ Matcher service validates:
  âœ”ï¸ Whether User A is matched with User B
â¤ Communicates with Session Service
â¤ Prevents unauthorized messaging


ğŸ” Match Validation Flow
1ï¸âƒ£ Message request sent
2ï¸âƒ£ Matcher validates match
3ï¸âƒ£ Session service resolves connection
4ï¸âƒ£ Message delivered


ğŸ” Handling App Reinstall Scenario
â¤ Matcher service stores all matches
â¤ Client pulls matches on reinstall
â¤ Only data lost:
  âœ”ï¸ Left / Right swipes
â¤ Acceptable trade-off


ğŸ” Recommendation Engine (Final Requirement)
â“ Core Problem
â¤ Recommend users nearby

ğŸ” Data Required for Recommendations
âœ”ï¸ Age
âœ”ï¸ Gender
âœ”ï¸ Location


ğŸ” Indexing Misconception 
â¤ Cannot efficiently optimize queries on multiple indexes simultaneously
â¤ Database uses **only one index per query**
â¤ Query optimizer decision is unpredictable


ğŸ” Why Traditional RDBMS Struggles Here
â¤ Multiple filters:
  âœ”ï¸ age range
  âœ”ï¸ gender
  âœ”ï¸ location proximity
â¤ Only one index effectively used
â¤ Poor performance at scale

âœ… Solution: NoSQL for Recommendations
1ï¸âƒ£ Cassandra (Exampleâ¤ 
â¤ Distributed NoSQL database
â¤ Data replication allowed
â¤ Tables designed per query pattern

ğŸ§  Key Idea
â¤ Duplicate data
â¤ Optimize for **read patterns**, not normalization


ğŸ” Recommendation DB Direction
âœ”ï¸ Distributed database (e.g., Cassandra)
âœ”ï¸ Multiple tables per query type
âœ”ï¸ Enables efficient multi-parameter filtering


ğŸ” Recommendation Storage â€“ Two Approaches
1ï¸âƒ£ Distributed NoSQL Databases
âœ… Examples:
  âœ”ï¸ Cassandra
  âœ”ï¸ Amazon DynamoDB

 â¤Built-in support for:
  âœ”ï¸ Distribution
  âœ”ï¸ Replication
  âœ”ï¸ High availability
â¤ Query-optimized table design

2ï¸âƒ£ Sharding on Relational Databases
â¤ Also called **Horizontal Partitioning**
â¤ Data split across nodes based on a column value

âœ… Sharding Explained with Example
â¤Partition key: name
â¤Users with names:
  âœ”ï¸ Aâ€“J â†’ DB Node 36
  âœ”ï¸ Kâ€“P â†’ DB Node 79
â¤ Query routing:
  âœ”ï¸ Value determines which node to hit

â“ Why Sharding Helps
âœ”ï¸ Limits query scope to specific nodes
âœ”ï¸ Improves performance
âœ”ï¸ Scales relational databases


ğŸ§  Single Point of Failure Concern
â—Problem
â¤ If one shard crashes, users mapped to it are affected

âœ… Solution
â¤ Masterâ€“Slave replication per shard
â¤ Failover strategy:
  âœ”ï¸ Master fails â†’ Slave promoted
â¤ Low probability of both failing simultaneously


ğŸ”„ Sharding ğŸ†š Cassandra 
â¤ Sharding:
  âœ”ï¸ More control
  âœ”ï¸ More operational complexity
â¤ Cassandra / Dynamo:
  âœ”ï¸ Built-in sharding + replication
  âœ”ï¸ Faster to reason about in interviews


ğŸ” Why Location-Based Sharding is Needed
â¤ Core recommendation problem: Who is nearby?
â¤ Partition users based on:
  âœ”ï¸ Location chunks (not necessarily city-level)


ğŸ” Location Chunking Strategy
â¤ Divide geographic area into chunks
â¤ Each chunk â†’ mapped to a shard/node
â¤ Query flow:
  âœ”ï¸ Fetch users from same chunk
  âœ”ï¸ Filter by age
  âœ”ï¸ Filter by gender


ğŸ” Recommendation Engine Design
ğŸ”„ Recommendation Service
â¤ Separate service
â¤ Responsibilities:
  âœ”ï¸ Pull nearby users
  âœ”ï¸ Apply filters


ğŸ” Location Updates
â¤ Client periodically updates location
â¤ Frequency:
  âœ”ï¸ Every 1â€“3 hours
â¤ Avoid excessive writes


ğŸ” Recommendation Data Storage
â¤ Store minimal data:
  âœ”ï¸ user_id
  âœ”ï¸ location
â¤ Profile service can provide additional info


ğŸ” Final Interview Pointers
â¤ Break system into independent services
â¤ Remove single points of failure
â¤ Prefer partitioning + replication
â¤ Simple systems are valid systems
â¤ Tinder is simpler than feed-based systems

