ğŸ” Problem Statement (Why Masterâ€“Slave?)
â¤ Clients â†’ Load Balancer â†’ Multiple App Servers â†’ Single Database
â¤ Single Point of Failure (SPOF):
   âœ”ï¸ If DB crashes â†’ entire system stops
â¤ Goal:
   âœ”ï¸ Improve availability
   âœ”ï¸ Improve read scalability
   âœ”ï¸ Provide fault tolerance


ğŸ” Core Idea: Replication
â¤ Maintain copies of the database
â¤ Copies stored on different hardware (different machines / disks / racks)
â¤ If one DB fails â†’ system can still serve requests


ğŸ” Replication Models
ğŸ”„ Asynchronous Replication
â¤ Master writes data
â¤ Slave copies data later

âœ… Pros
â¤ Low write latency
â¤ Less load on master

âŒ Cons
â¤ Data inconsistency possible
â¤ Data loss if master crashes before sync

Use cases
â¤ Social media comments
â¤ Analytics
â¤ Non-critical reads


ğŸ”„ Synchronous Replication
â¤ Master write completes only after slave confirms

âœ… Pros
â¤ Strong consistency
â¤ No data loss

âŒ Cons
â¤ Higher write latency
â¤ Slower system

ğŸ”„ Reality check
â¤ True sync does not exist automatically
â¤ Enforced via transaction logs + protocols


ğŸ” Master and Slave Roles
ğŸ”„ Master
â¤ Accepts:
   âœ”ï¸ Writes
   âœ”ï¸ Reads
â¤ Maintains transaction log
â¤ Sends write commands to slaves

ğŸ”„ Slave
â¤ Accepts:
   âœ”ï¸ Reads only (in pure masterâ€“slave)
â¤ Replays masterâ€™s commands in the same order
â¤ Achieves same final state

ğŸ‘‰ Key Rule: Any database accepting writes is a Master


ğŸ” Write Restriction Rule
ğŸ”„ Why slaves should not accept writes?
â¤ Prevents conflicts
â¤ Ensures single source of truth

ğŸ”„ Two Options:
1ï¸âƒ£ âŒ Never allow writes on slaves (classic masterâ€“slave)
2ï¸âƒ£ âš ï¸ Allow writes â†’ becomes Masterâ€“Master


ğŸ” Masterâ€“Master (Peer-to-Peer) Architecture
ğŸ”„ Characteristics
â¤ Multiple nodes accept writes
â¤ Data propagates bidirectionally
â¤ Appears:
   âœ”ï¸ Highly available
   âœ”ï¸ Load-balanced for writes

ğŸ”„ Hidden Danger
â¤ Split Brain Problem


ğŸ” Split Brain Problem
â¤ Nodes lose network connectivity
â¤ Each node assumes it is the master
â¤ Both accept writes independently

âœ… Example (Bank Account)
 Balance = 120
 Request to A: deduct 100
 Request to B: deduct 50
 Result: balance = -30 âŒ

ğŸ”„ Root Cause
â¤ Network partition â‰  node failure


ğŸ” Solving Split Brain: Adding a Third Node
ğŸ”„ Quorum-Based Approach
â¤ Nodes: A, B, C
â¤ Writes require majority agreement

ğŸ”„ Assumption
â¤ Node failure + network failure together is rare


ğŸ” State-Based Validation
ğŸ”„ Initial State
 A = S0, B = S0, C = S0

ğŸ”„ Scenario
â¤ A updates â†’ SX â†’ propagated to C
â¤ B updates â†’ SY â†’ asks C
â¤ C rejects because:
    âœ”ï¸ Previous state mismatch

ğŸ”„ Outcome
â¤ B transaction rolls back
â¤ B syncs to SX
â¤ Client retries safely


ğŸ” Transactions & Rollbacks
â¤ State changes are tentative
â¤ Commit happens only after:
   âœ”ï¸ All required nodes acknowledge
â¤ Prevents permanent corruption


ğŸ” Distributed Consensus Multiple nodes agree on a single value/state

ğŸ”„ Popular Protocols
â¤ 2PC (Two-Phase Commit)
    âœ”ï¸ Simple
    âœ”ï¸ Very slow
    âœ”ï¸ Blocking

â¤ 3PC (Three-Phase Commit)
    âœ”ï¸ Safer than 2PC
    âœ”ï¸ Rarely used

â¤ Paxos / Raft (added knowledge)
    âœ”ï¸ Used in modern systems
    âœ”ï¸ Leader-based consensus


ğŸ” MVCC (Multi-Version Concurrency Control)
ğŸ”„ Used By: PostgreSQL

ğŸ”„ Idea
â¤ Maintain multiple versions of same data

ğŸ”„ Benefits
â¤ Non-blocking reads
â¤ Supports different isolation levels

ğŸ”„ Trade-off
â¤ More memory
â¤ Cleanup required (vacuum)


ğŸ” Saga Pattern (Long Transactions)
â¤ Transaction broken into smaller steps
â¤ Each step has a compensating action

ğŸ”„ Use Case 1: Food Ordering
â¤ Lock money
â¤ Confirm restaurant
â¤ Commit or rollback

ğŸ”„ Use Case 2: Phone Calls
â¤ Lock funds per minute
â¤ Final charge at end


ğŸ” Why Use Masterâ€“Slave Architecture?
1ï¸âƒ£ Backup & Fault Tolerance
â¤ Slave acts as hot standby

2ï¸âƒ£ Read Scalability
â¤ Reads served from slaves
â¤ Writes remain on master

3ï¸âƒ£ Analytics Isolation
â¤ Heavy queries run on slaves

4ï¸âƒ£ Eventual Consistency Support
â¤ Acceptable for feeds, comments


ğŸ” Scaling Further: Sharding + Replication

ğŸ”„ Sharding
â¤ Partition data by key range
âœ… Example:
    âœ”ï¸ A â†’ users 0â€“100
    âœ”ï¸ B â†’ users 100â€“200
    âœ”ï¸ C â†’ users 200â€“300

ğŸ”„ Benefits
â¤ Reduced blast radius
â¤ Horizontal scaling

ğŸ”„ High Availability
â¤ Each shard has a slave
â¤ Slave promoted on failure


ğŸ” Combined Architecture
â¤ Sharding â†’ scale
â¤ Replication â†’ availability
â¤ Consensus â†’ correctness

â¤ Masterâ€“Slave architecture improves availability and read scalability by separating write and read responsibilities.
â¤ Split brain occurs when multiple masters accept writes during network partition.
â¤ Consensus protocols ensure a single source of truth in distributed systems.


ğŸ” Real-World Examples (Added Knowledge)
â¤ MySQL Replication
â¤ PostgreSQL Streaming Replication
â¤ Redis Masterâ€“Replica
â¤ MongoDB Replica Sets


ğŸ” Mental Model
Masterâ€“Slave = Write consistency + Read scalability + Operational simplicity
