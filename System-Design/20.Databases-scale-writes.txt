ğŸ” Problem Context: Scaling Database Writes
â¤ Client sends data â†’ Server â†’ Database
â¤ Database must persist state reliably
â¤ Main challenges at scale:
  âœ”ï¸ Too many acknowledgments (requestâ€“response overhead)
  âœ”ï¸ Too many I/O calls (disk is slow)
  âœ”ï¸ High latency for writes


ğŸ” Traditional Database Approach
ğŸ”„ B+ Tree as Core Data Structure
â¤ Used by traditional relational databases
â¤ Characteristics:
  âœ”ï¸ Multi-way tree (not binary)
  âœ”ï¸ Optimized for disk-based storage
â¤ Time Complexity:
  âœ”ï¸ Insert â†’ O(log n)
  âœ”ï¸ Search â†’ O(log n)
â¤ Every SQL `INSERT` / `SELECT`:
  âœ”ï¸ Translates to B+ tree operations
  âœ”ï¸ Requires disk I/O + acknowledgment


ğŸ”„ Limitation
â¤ Too many small writes â†’ too many I/O calls
â¤ Each operation waits for DB acknowledgment


ğŸ” First Optimization Idea: Batch Writes
ğŸ”„ Concept
â¤ Condense multiple write queries into one block
â¤ Send to DB in one shot
â¤ Receive single acknowledgment


ğŸ”„ Advantages
â¤ Fewer I/O operations
â¤ Better bandwidth utilization
â¤ Lower requestâ€“response overhead


ğŸ”„ Drawback
â¤ Requires extra memory on server to buffer writes
â¤ Trade-off accepted (memory is cheaper than I/O)

ğŸ” Fastest Data Structure for Writes
ğŸ”„ Linked List (Log)
â¤ Append-only structure
â¤ Insert at tail â†’ O(1)

ğŸ”„ Key Insight
â¤ Log behaves like a linked list
â¤ Perfect for write-heavy workloads

ğŸ”„ Write Path
â¤ Writes are appended sequentially
â¤ Minimal disk seek


ğŸ” Major Problem with Logs
ğŸ”„ Read Performance
â¤ Sequential scan required
â¤ Search â†’ O(n)
â¤ Completely unacceptable at scale (e.g., Facebook feed)


ğŸ” Solution Direction
ğŸ”„ Goal
â¤ Keep:
  âœ”ï¸ Fast writes (log)
  âœ”ï¸ Reduced I/O
â¤ Avoid: Slow reads

ğŸ”„ Key Idea
â¤ Writes â†’ Log / Linked List
â¤ Reads â†’ Sorted structure (binary searchable)


ğŸ” Sorted String Table (SSTable)
â¤ Immutable, sorted set of key-value records
â¤ Stored on disk

ğŸ”„ Process
1ï¸âƒ£ Writes go to in-memory log
2ï¸âƒ£ When threshold is reached:
   âœ”ï¸ Data is sorted
   âœ”ï¸ Persisted to disk as an SSTable

ğŸ”„ Benefits
â¤ Binary search possible
â¤ Read â†’ O(log n)


ğŸ” Multiple SSTables Problem
ğŸ”„ Scenario
â¤ Multiple flushes create multiple SSTables
â¤ Each SSTable is sorted

ğŸ”„ Read Cost
â¤ Search every SSTable
â¤ Time â‰ˆ (number of SSTables) Ã— log(size)
â¤ Still slow at scale


ğŸ” 9. Compaction
â¤ Background process
â¤ Merges multiple SSTables into larger SSTables

ğŸ”„ Merge Logic
â¤ Similar to merge step of merge sort
â¤ Two sorted arrays merged using two pointers

âœ… Example
â¤ Two SSTables of size 6 â†’ one SSTable of size 12
â¤ Two SSTables of size 12 â†’ one SSTable of size 24


ğŸ” Compaction Strategy
ğŸ”„ Size-Tiered Compaction
â¤ Merge SSTables of same size
â¤ Growth pattern:
  âœ”ï¸ 6 â†’ 12 â†’ 24 â†’ 48 â†’ â€¦ â†’ n

ğŸ”„ Benefit
â¤ Reduces number of SSTables
â¤ Reduces read amplification


ğŸ” Read Complexity Comparison
1ï¸âƒ£ Without Compaction
â¤ Many small SSTables
â¤ Read cost â‰ˆ k Ã— log(m)

2ï¸âƒ£ With Compaction
â¤ Few large SSTables
â¤ Read cost â‰ˆ log(total records)


ğŸ” Bloom Filters
â¤ Avoid unnecessary SSTable reads

ğŸ”„ Properties
â¤ Probabilistic data structure
â¤ Can have false positives
â¤ Cannot have false negatives


ğŸ”„ Usage in LSM(Log-Structured Merge) Tree
â¤ Each SSTable has its own Bloom filter
â¤ Before searching SSTable:
  âœ”ï¸ Check Bloom filter
  âœ”ï¸ If false â†’ skip SSTable entirely


ğŸ” Bloom Filter Tuning
â¤ Larger SSTable â†’ larger Bloom filter
â¤ More bits â†’ lower false positive rate
â¤ Reduces disk reads


ğŸ” End-to-End LSM Tree Workflow
1ï¸âƒ£ Writes appended to in-memory log (O(1))
2ï¸âƒ£ Log reaches threshold
3ï¸âƒ£ Data sorted â†’ written as SSTable
4ï¸âƒ£ Multiple SSTables exist
5ï¸âƒ£ Background compaction merges SSTables
6ï¸âƒ£ Bloom filters speed up reads


ğŸ” Key Terminology
â¤ Log: Append-only write structure
â¤ MemTable: In-memory sorted buffer (implied)
â¤ SSTable (Sorted String Table): Immutable sorted disk table
â¤ Compaction: Merging SSTables
â¤ Bloom : Probabilistic read-avoidance structure
â¤ Write : Extra writes due to compaction
â¤ Read : Multiple SSTables per read


ğŸ” Why LSM Trees Are Use
â¤ Write-heavy systems
â¤ Sequential disk writes
â¤ High throughput


ğŸ”„ Real-World Systems
â¤ Cassandra
â¤ RocksDB
â¤ LevelDB
â¤ HBase
