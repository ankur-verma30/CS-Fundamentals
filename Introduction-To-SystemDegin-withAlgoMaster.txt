============================
ğŸ”· What Is System Design?
============================
â¤ At its core, System Design is the process of defining how different parts of a software system interact to meet both functional (what it should do) and non-functional (how well it should do it) requirements.
â¤ Itâ€™s not about writing code, at least not yet. Itâ€™s about making high-level architectural decisions that balance scalability, reliability, performance, and cost.


ğŸ”„ Real-World Analogy: Designing a Skyscraper
â¤ Imagine youâ€™re an architect designing a skyscraper.
â¤ You donâ€™t start by laying bricks. You start by asking questions:

â“ How many floors will it have?
â“ How many people should it support?
â“ What kind of soil is it built on?
â“ What level of earthquake resistance is needed?

âœ”ï¸ Once the requirements are clear, you create blueprints showing how everything fits together: the foundation, the structural supports, the plumbing, the electrical layout, and the elevator shafts.
âœ”ï¸ You also consider how different systems interact, such as how plumbing might affect electrical layouts.
âœ”ï¸ You plan for future expansion (scalability) and think about how the building will handle unexpected issues (fault tolerance).


ğŸ‘‰ In the software world, this translates to:
â¤ Architecture â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ The overall structure of the system. Should the system be built as a monolith, a set of microservices, or an event-driven system?
â¤ Components/Modules â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Databases, servers, load balancers, caches, message queues, and APIs.
â¤ Interfaces â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How these components communicate with each other
(e.g., REST APIs, gRPC).
â¤ Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How data is stored, managed, accessed, and kept consistent.


======================================
ğŸ”· 10 Big Questions of System Design
======================================
On a high level, system design revolves around answering these 10 big questions:
1ï¸âƒ£ Scalability â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How will the system handle a large number of users or requests simultaneously?
2ï¸âƒ£ Latency and Performance â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How can we reduce response time and ensure low-latency performance under load?
3ï¸âƒ£ Communication â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How do different components of the system interact with each other?
4ï¸âƒ£ Data Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How should we store, retrieve, and manage data efficiently?
5ï¸âƒ£ Fault Tolerance and Reliability â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ What happens if a part of the system crashes or becomes unreachable?
6ï¸âƒ£ Security â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How do we protect the system against threats such as unauthorized access, data breaches, or denial-of-service attacks?
7ï¸âƒ£ Maintainability and Extensibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How easy is it to maintain, monitor, debug, and evolve the system over time?
8ï¸âƒ£ Cost Efficiency â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How can we balance performance with infrastructure cost?
9ï¸âƒ£ Observability and Monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How do we monitor system health and diagnose issues in production?
1ï¸âƒ£0ï¸âƒ£ Compliance and Privacy â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Are we complying with relevant laws and regulations (e.g., GDPR, HIPAA)?


============================
ğŸ”· Key Components of System
=============================
A typical software system can be broken down into several key components:

1ï¸âƒ£ Client/Frontend â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ The part of the system that users interact with directly (e.g., web browsers, mobile apps). It is responsible for displaying information, collecting user input, and communicating with the backend.

2ï¸âƒ£ Server/Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ The backend handles the core functionality of the system. It processes incoming requests, executes business logic, interacts with databases or services, and sends responses back to the client.

3ï¸âƒ£ Database/Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ This component is responsible for storing and managing data. It can take various forms, including relational databases (SQL), non-relational stores (NoSQL), in-memory caches, or distributed object storage systems, depending on the needs of the application.

4ï¸âƒ£ Networking Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ This includes components like load balancers, APIs, and communication protocols that ensure reliable and efficient interaction between different parts of the system.

5ï¸âƒ£ Third-party Services â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ These are external APIs or platforms that extend the systemâ€™s capabilities. Common examples include payment processors, email or SMS notification services, authentication providers, analytics tools, and cloud-based AI services.


============================
ğŸ”· Process of System Design
=============================
Designing a system is not a one-size-fits-all approach. Itâ€™s a step-by-step process that starts with understanding the requirements and ends with a detailed blueprint.

Here are the key steps:
Step 1ï¸âƒ£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Requirements Gathering
Every design starts with a conversation. Before drawing diagrams or choosing technologies, focus on understanding what the system needs to do.

Ask questions like:
â“ What are the functional requirements (core features and workflows)?
â“ What are the non-functional requirements (scalability, availability, latency, consistency)?
â“ Who are the users, and how many are expected initially and at scale?
â“ Whatâ€™s the expected data volume and traffic pattern?
â“ Are there any constraints (e.g., specific technologies, budgets, or compliance rules)


Step 2ï¸âƒ£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Back-of-the-Envelope Estimation
Next, estimate the scale of your system. Approximate numbers give you a sense of what youâ€™re designing for.

Estimate:
âœ”ï¸ Data size (storage requirements)
âœ”ï¸ Queries per second (QPS) or requests per second (RPS)
âœ”ï¸ Bandwidth needs
âœ”ï¸ Number of servers or instances required
ğŸ‘‰ These rough calculations help guide architectural decisions and ensure your design is grounded in realistic expectations.


Step 3ï¸âƒ£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ High-Level Design (HLD)
Now that you understand what youâ€™re building and how big it needs to be, start visualizing the systemâ€™s core components and how they interact.

Define:
âœ”ï¸ The main modules and services
âœ”ï¸ Data flow between them
âœ”ï¸ External dependencies (e.g., third-party APIs, external databases)
ğŸ‘‰ At this stage, youâ€™re sketching the architecture blueprint, a birdâ€™s-eye view of the system.


Step 4ï¸âƒ£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Data Model / API Design
Once the architecture is clear, move closer to the data and interfaces.

âœ”ï¸ Choose the right database type(s) â€” relational, NoSQL, time-series, etc.
âœ”ï¸ Define schemas, tables, and relationships to support your use cases.
âœ”ï¸ Design APIs for interaction between services (e.g., POST /tweet, GET /timeline).


Step 5ï¸âƒ£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Detailed Design / Deep Dive
Zoom into each component and define:
âœ”ï¸ Internal logic, caching, and concurrency handling
âœ”ï¸ Scaling strategies (horizontal vs vertical scaling)
âœ”ï¸ Replication, partitioning, and fault tolerance

This is also where you address non-functional requirements (NFRs) like availability, reliability, and latency.
ğŸ‘‰ In other words, you go from what the system does to how it does it.


Step 6ï¸âƒ£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Identify Bottlenecks and Trade-offs
No system is perfect. Every choice has trade-offs.

Ask yourself:
â“ Where could the system break under high load?
â“ What are the single points of failure?
â“ Can caching or replication help reduce pressure?
â“ Is it okay to choose eventual consistency for higher availability?

ğŸ‘‰ A strong design doesnâ€™t eliminate trade-offs, it makes them explicit and justifiable.


Step 7ï¸âƒ£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Review, Explain, and Iterate
âœ”ï¸ Finally, step back and evaluate. Explain your design decisions clearlyâ€”why you made certain choices and how they meet the requirements.
âœ”ï¸ Be open to feedback, iterate on weak spots, and refine your design.
âœ”ï¸ You donâ€™t need to get everything perfect on the first try. What matters is your ability to adapt, refine, and evolve the design as you uncover new insights or constraints.


===========================
ğŸ”· System Design Concepts
===========================
1ï¸âƒ£ Networking Foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How computers talk to each other
2ï¸âƒ£ APIs and Communication â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ How applications exchange data
3ï¸âƒ£ Data Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Where and how data lives
4ï¸âƒ£ Scaling â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Handling growth
5ï¸âƒ£ Distributed Systems â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Challenges of running multiple machines
6ï¸âƒ£ Architecture Patterns â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Organizing large-scale systems


===========================
ğŸ”· Networking Foundations
===========================
Before designing any system, you need to understand how computers talk to each other. Every single request in a distributed system, whether it is a Google search or an Instagram like, travels across a network.

These six concepts are the foundation everything else rests on.
1ï¸âƒ£ Client-Server Model
â¤ Every web application you use follows a simple pattern: your browser (the client) asks a server for something, and the server responds.
â¤ That is the client-server model. It is so fundamental that you will see it in literally every system design diagram.

+------------------+           Request            +----------------------+
|                  | --------------------------> |                      |
|      CLIENT      |                             |        SERVER        |
|  (Browser / App) | <-------------------------- |  (API / Backend)     |
|                  |           Response          |                      |
+------------------+                             +----------+-----------+
                                                             |
                                                             |
                                                             v
                                                   +----------------------+
                                                   |                      |
                                                   |      DATABASE        |
                                                   |   (MySQL / NoSQL)    |
                                                   |                      |
                                                   +----------------------+

â¤ The client is any device that initiates a request: your browser, a mobile app, or even another server.
â¤ The server listens for requests, processes them, and sends back responses.
â¤ This separation is powerful because it lets you change the client (build a mobile app alongside the web app) without touching the server, and scale the server independently of clients.

â“ But how does the client actually find the server on the internet?


2ï¸âƒ£ IP(Internet Protocol) Address
â¤ Every device connected to the internet has a unique address, just like every house has a street address.
â¤ That is an IP address. Without it, your request would have no idea where to go.

+------------------+   Packet   +------------------+   Routes to  +------------------+   Delivers to +------------------+
|   Your Device    | --------> |      Router      | ------------> |     Internet     | -------------> |      Server      |
| 192.168.1.10     |           |    Gateway       |               |    Backbone      |                | 142.250.80.46    |
+------------------+           +------------------+               +------------------+                +------------------+

There are two versions:
    âœ”ï¸ IPv4 (like 192.168.1.1) and IPv6 (like 2001:0db8:85a3::8a2e:0370:7334).
    âœ”ï¸ IPv4 gives us about 4.3 billion addresses, which seemed like plenty until every smartphone, laptop, and smart fridge needed one.
    âœ”ï¸ IPv6 solves this by offering a virtually unlimited number of addresses.

â¤ For system design, the key thing to know is that IP addresses identify machines on a network.
â¤ When you talk about servers, load balancers, or database nodes, each one has an IP address that others use to reach it.


â“ Of course, nobody wants to memorize IP addresses. So how does your browser know that "google.com" maps to 142.250.80.46?


3ï¸âƒ£ DNS (Domain Name System)
â¤ DNS is the phone book of the internet. You type "google.com" into your browser, and DNS translates that into an IP address your computer can actually route to.
â¤ Without DNS, you would need to memorize the IP address of every website you visit.


+------------------+   Query / Ask   +---------------------+   Try   +--------------+   Try .com   +------------------+   Try google   +-----------------------+   IP Address   +------------------+
|     Browser      | --------------> |    DNS Resolver     | ------> |  Root Server  | ----------> |   TLD Server     | ------------> |  Authoritative NS     | ------------> |  IP: 142.250.80.46 |
|   google.com     |                 |     ISP / Local     |         |               |             |      .com        |               |                       |               |                    |
+------------------+                 +---------------------+         +--------------+             +------------------+               +-----------------------+               +------------------+


â¤ The resolution process involves multiple servers working together.
â¤ Your browser first checks its local cache. If the answer is not there, it asks a DNS resolver (usually provided by your ISP).
â¤ The resolver queries root servers, then top-level domain servers (.com, .org), and finally the authoritative name server for that specific domain.
â¤ The result gets cached at every level so this chain does not repeat for every request.

â¤ DNS is also used for load balancing (returning different IPs for the same domain) and failover (pointing traffic to a backup server when the primary goes down).

ğŸ‘‰ Now that the client can find the server, your request doesn't always go directly to the server.


4ï¸âƒ£ Proxy vs Reverse Proxy
ğŸ”„ Forword Proxy
+-----------+     Request     +----------------+     Request     +----------------+
|  Client   |  ------------> |  Forward Proxy |  ------------> |  Internet /    |
| (User)    |                |                |                |  Server        |
+-----------+                +----------------+                +----------------+
      ^                             |
      |         Response            |
      +-----------------------------+

A forward proxy sits in front of clients. It hides the client's identity from the server.
Think of a VPN or corporate proxy: the server sees the proxy's IP, not yours. Use cases include privacy, content filtering, and bypassing geo-restrictions.


ğŸ”„ Reverse Proxy
+-----------+     Request     +----------------+     Request     +------------------+
|  Client   |  ------------> | Reverse Proxy  |  ------------> |  Backend Server  |
| (User)    |                |                |                |  (App Servers)   |
+-----------+                +----------------+                +------------------+
      ^                             |
      |         Response            |
      +-----------------------------+

â¤ A reverse proxy sits in front of servers. It hides the server's identity from the client.
â¤ The client thinks it is talking to one server, but the reverse proxy could be routing requests to dozens of backend servers.
â¤ Nginx and HAProxy are common examples. Reverse proxies handle SSL termination, caching, compression, and load balancing.


ğŸ‘‰ Whenever a client communicates with a server, thereâ€™s always some delay. One of the biggest causes of this delay is physical distance and that's latency.


5ï¸âƒ£ Latency and Bandwidth
â¤ Latency is the time it takes for data to travel from point A to point B. It is measured in milliseconds and it directly impacts user experience.
â¤ A 100ms delay is barely noticeable, but 1 second feels sluggish, and 3 seconds? Users start leaving.


+-----------+   Transmission Delay   +-----------+   Propagation Delay   +-----------+   Processing Delay   +-----------+   Queuing Delay   +-----------+
|  Client   | --------------------> |  Network  | --------------------> |  Router   | ------------------> |  Router   | ----------------> |  Server   |
+-----------+                       +-----------+                       +-----------+                     +-----------+                   +-----------+

â¤ Latency comes from multiple sources: network distance (speed of light in fiber), serialization (converting data to bytes), processing time on the server, and queuing delays when the server is busy.
â¤ You cannot beat physics, so a request from Mumbai to New York will always take longer than Mumbai to a nearby server.
â¤ This is why system design solutions often include CDNs (serving content from nearby edge nodes), caching (avoiding round trips to the database), and regional deployments (placing servers closer to users).

ğŸ‘‰ Reducing latency is one of the most common non-functional requirements in system design.

Now, when data actually travels between client and server, what language do they speak?


6ï¸âƒ£ Protocols: HTTP/ HTTPS
â¤ HTTP (Hypertext Transfer Protocol) is the language that clients and servers use to communicate on the web.
â¤ It defines how requests are structured and how responses come back.
â¤ HTTPS is the same thing, but encrypted with TLS so nobody can eavesdrop.


+-----------+   Client Hello    +-----------+
|  Browser  | ----------------> |  Server   |
| (Client)  |                   |           |
+-----------+                   +-----------+
       |                               |
       |   Server Hello + Certificate |
       | <----------------------------|
       |                              |
       |  Certificate Verification    |
       |  (CA / Trust Store)          |
       |                              |
       |  Pre-Master Secret           |
       |  (Encrypted with Public Key) |
       | ---------------------------->|
       |                              |
       |  Session Key Generated       |
       | <----------------------------|
       |                              |
       |====== Secure TLS Tunnel ====>|
       |                              |
       |  HTTPS GET Request           |
       |  (Encrypted HTTP Header)     |
       | ---------------------------->|
       |                              |
       |  HTTPS Response (Data)       |
       | <----------------------------|
       |                              |
       |  HTTPS POST Request          |
       |  (Encrypted Header + Body)   |
       | ---------------------------->|
       |                              |
       |  HTTPS Response (Status/Data)|
       | <----------------------------|
       |                              |

â¤ HTTP is stateless: each request is independent, and the server does not remember previous requests.
â¤ This makes scaling easier (any server can handle any request) but means you need mechanisms like cookies, tokens, or sessions to maintain state across requests.

â“ Key things to know for interviews: HTTP methods (GET, POST, PUT, DELETE), status codes (200 OK, 404 Not Found, 500 Server Error), and headers (for authentication, caching, content type).
â¤ HTTPS adds a TLS handshake that takes an extra round trip but protects data in transit.

ğŸ‘‰ With networking covered, let's move up a layer. How do applications actually exchange data in a structured way?


===========================
ğŸ”· APIs and Communication
===========================
Now that we know how machines connect, how do applications actually talk to each other? You need well-defined contracts, called APIs, that specify what data you can request and what format it comes back in.

This group covers the major API styles and real-time communication patterns.
1ï¸âƒ£  APIs (Application Programming Interfaces)
An API is a contract between two pieces of software. It defines what you can ask for, how to ask for it, and what you will get back.

+-------------+   JSON Request   +-------------+   Route to   +---------------------------+   Process   +------------------+
|  Client App | --------------> |  API Layer  | -----------> | Authentication & Validation | ---------> |  Business Logic  |
+-------------+                 | Endpoints   |              +---------------------------+            +------------------+
       ^                         +-------------+                                                          |
       |                                                                                                    |
       |--------------------------------------- JSON Response ---------------------------------------------+


ğŸŸ¢ Real-World Analogy:
Think of an API like a restaurant menu. You do not go into the kitchen and cook. You look at the menu (API documentation), place an order (request), and get your food (response). The kitchen (server) handles the complexity behind the scenes.

â¤ Most modern APIs communicate using JSON over HTTP.
â¤ The client sends a request to a specific URL (called an endpoint), the server processes it, and returns a response with a status code and data.
â¤ But, not all APIs are built the same. Different API styles exist to serve different needs. Two of the most popular ones are REST and GraphQL.


2ï¸âƒ£ REST (Representational State Transfer)
â¤ REST (Representational State Transfer) is the most common API style on the web.
â¤ It treats everything as a "resource" and uses standard HTTP methods to interact with those resources.
â¤ It is simple, stateless, and works well for most applications.

+-------------+   HTTP Request   +------------------+   Route to   +-------------------------+   Execute   +------------------+
|  Client App | --------------> |   REST API       | -----------> | Authentication &        | ---------> |  Business Logic  |
| (Web/Mobile)|                 |   Controller     |              | Authorization &         |            |  (Service Layer) |
+-------------+                 +------------------+              | Validation              |            +------------------+
       ^                                                                            |
       |                                                                            |
       |-------------------------------------- HTTP Response (JSON/XML) -----------+

â¤ REST maps naturally to CRUD operations: Create (POST), Read (GET), Update (PUT/PATCH), and Delete (DELETE).
â¤ Each resource gets its own URL, and the HTTP method tells the server what to do.
âœ… For example, GET /users/123 fetches user 123, while DELETE /users/123 removes them.

Key REST principles:
    âœ”ï¸ stateless (no session on the server)
    âœ”ï¸ cacheable (responses can be cached)
    âœ”ï¸ uniform interface (consistent URL patterns)
REST's simplicity is its biggest strength, but it has a drawback: if a client needs data from multiple resources, it has to make multiple requests.

â“ What if the client could get exactly the data it needs in a single request?


3ï¸âƒ£ GraphQL
GraphQL, developed by Facebook, lets clients request exactly the data they need in one query.
Instead of hitting multiple REST endpoints, you send a single query describing the shape of the data you want, and the server returns exactly that.

+-------------+   GraphQL Query   +------------------+   Parse & Validate   +------------------+   Fetch Data   +------------------+
|  Client App | ----------------> |  GraphQL Server  | -------------------> |     Schema       | ------------> |    Resolvers     |
| (UI / App)  |                  |   (Single API)   |                      | (Type System)   |               | (Business Logic)|
+-------------+                  +------------------+                      +------------------+               +------------------+
       ^                                                                                                             |
       |                                                                                                             |
       |--------------------------------------------- GraphQL Response ---------------------------------------------+

â¤ With REST, fetching a user's profile with their posts might require two separate requests: one to /users/123 and another to /users/123/posts.
â¤ With GraphQL, you describe both in a single query, and the server resolves them and returns a combined result.

â¤ The trade-off? GraphQL adds complexity on the server side (you need resolvers for each field), can make caching harder (since every query is different), and can lead to performance issues if clients request deeply nested data.

â¤ For most system design problems, REST is the safer default, but mentioning GraphQL shows breadth of knowledge, especially for systems where clients need flexible data fetching (like social media feeds).

REST and GraphQL handle request-response patterns. But what about real-time communication where the server needs to push data to clients?


4ï¸âƒ£ WebSockets
HTTP is a one-way street: the client always initiates the request. But what about chat apps, live sports scores, or collaborative editors?

You need the server to push updates to the client instantly. That is where WebSockets come in.

CLIENT SIDE                                  SERVER SIDE
-----------                                  -----------
+----------------------+                     +----------------------+
|        Client        |                     |        Server        |
|   (Browser / App)    |                     |  (WebSocket Server) |
+----------+-----------+                     +----------+-----------+
           |                                            |
           |  HTTP Upgrade Request                      |
           |  (Handshake)                               |
           |------------------------------------------->|
           |                                            |
           |  101 Switching Protocols                   |
           |  (Upgrade to WebSocket)                    |
           |<-------------------------------------------|
           |                                            |
+----------+-----------+                     +----------+-----------+
|  WebSocket Connection |<==================>|  WebSocket Connection |
|   (Persistent)        |   Full-Duplex      |   (Persistent)        |
+----------+-----------+                     +----------+-----------+
           |                                            |
           |  Message Frame (Text / Binary)             |
           |------------------------------------------->|
           |                                            |
           |  Message Frame (Text / Binary)             |
           |<-------------------------------------------|
           |                                            |
+----------+-----------+                     +----------+-----------+
| Application Logic     |                     | Application Logic     |
| (Chat / Notifications)|                     | (Broadcast / Process) |
+----------------------+                     +----------------------+

â¤ WebSockets start as a regular HTTP request, then "upgrade" to a persistent, bidirectional connection.
â¤ Once established, both sides can send messages at any time without the overhead of creating new HTTP connections.
â¤ This makes them perfect for real-time features.

â¤ The downside is that WebSocket connections are stateful, the server needs to keep track of every open connection, which makes scaling harder.
â¤ If a server goes down, all its connections are lost.

â¤ WebSockets enable real-time communication between a client and a server, but what if a server needs to notify another server (or client) when an event occurs?

âœ… Example:
When a user makes a payment, Stripe needs to notify your application instantly.
If someone pushes code to GitHub, a CI/CD system (e.g., Jenkins) should be triggered automatically.
This is where Webhooks come in.


5ï¸âƒ£ Webhooks
â¤ Webhooks flip the usual model around. Instead of the client repeatedly asking "has anything changed?" (polling), the server sends a notification to the client when something happens.
â¤ The client registers a callback URL, and the server POSTs to that URL whenever the event occurs.

EVENT SOURCE / SENDER                     RECEIVER / LISTENER
----------------------                    ----------------------
+----------------------+                  +----------------------+
|   Source Application |                  |   Receiver Server    |
|  (GitHub / Stripe)   |                  |   (Webhook Endpoint) |
+----------+-----------+                  +----------+-----------+
           |                                           |
           |  Event Occurs                             |
           |  (Push, Payment, Commit)                  |
           |------------------------------------------>|
           |                                           |
+----------+-----------+                               |
|  Webhook Configured  |                               |
|  (Callback URL)      |                               |
+----------+-----------+                               |
           |                                           |
           |  HTTP POST Request                        |
           |  (JSON Payload + Signature)               |
           |------------------------------------------>|
           |                                           |
                                                +------+-----------+
                                                |  Authentication / |
                                                |  Signature Verify |
                                                +------+-----------+
                                                       |
                                                +------+-----------+
                                                |   Business Logic |
                                                |  (Process Event) |
                                                +------+-----------+
                                                       |
                                                +------+-----------+
                                                |   Response (2xx) |
                                                |   Acknowledgment |
                                                +------------------+

â¤ Webhooks are widely used for integrations: payment notifications (Stripe), code push events (GitHub), message delivery status (Twilio).
â¤ The key challenge is reliability, what if your server is down when the webhook fires? Good webhook systems include retry logic, event logging, and idempotency handling.
â¤ In system design, use webhooks for asynchronous event notifications between services, especially when integrating with third-party platforms.

â“ We have covered how data moves between applications. But where does all that data actually live?


===========================
ğŸ”· Data Storage
===========================
APIs move data around, but where does that data actually live? Choosing the right storage strategy is one of the most important decisions in system design. This group covers database types, indexing, partitioning, caching, and more.

1ï¸âƒ£ Databases
â¤ A database is an organized collection of data that supports efficient storage, retrieval, and manipulation.
â¤ But there is no single "best" database. Different data shapes and access patterns call for different types.

                                  +----------------------+
                                  |       DATABASE       |
                                  +----------+-----------+
                                             |
        --------------------------------------------------------------------------------
        |                                 |                                   |
+---------------------+         +---------------------+              +----------------------+
|   Relational DB     |         |    NoSQL DB         |              |   Specialized DB     |
|   (SQL Databases)   |         |  (Non-Relational)   |              |                      |
+----------+----------+         +----------+----------+              +----------+-----------+
           |                               |                                      |
    +------+-------+        ---------------------------------          ------------------------
    |              |        |        |        |        |                |          |          |
+-------+     +---------+ +--------+ +--------+ +--------+ +--------+ +---------+ +--------+
| Tables|     | Schema  | | Key-    | | Document| | Column | | Graph  | | In-Memory| | Time   |
| Rows  |     | Fixed   | | Value   | | Store   | | Store  | | DB     | | DB       | | Series |
+-------+     +---------+ +--------+ +--------+ +--------+ +--------+ +---------+ +--------+
                                  |
                         -------------------------
                         |          |            |
                   +----------+ +----------+ +----------+
                   | MongoDB  | | Cassandra| | Redis    |
                   +----------+ +----------+ +----------+


âœ”ï¸ Relational databases â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ (PostgreSQL, MySQL) store data in structured tables with defined relationships. Great for transactional data where consistency matters.

âœ”ï¸ Document databases â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ (MongoDB) store data as flexible JSON-like documents. Good for varied or evolving schemas.

âœ”ï¸ Key-value â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ stores (Redis, DynamoDB) are like hash maps, blazing fast for simple lookups by key.

âœ”ï¸ Graph databases â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ (Neo4j) model relationships as first-class citizens, ideal for social networks or recommendation engines.

â¤ In system design, we choose database based on the data model, query patterns, and scalability requirements.
â¤ There is no right answer without context.

The most common choice is between relational and non-relational databases. Let's dig deeper into that decision.


2ï¸âƒ£ SQL vs NoSQL
â¤ SQL databases enforce a strict schema and support ACID transactions.
â¤ NoSQL databases trade some of that rigidity for flexibility and horizontal scalability.

                           +----------------------+
                           |       DATABASE       |
                           +----------+-----------+
                                      |
                    -----------------------------------------
                    |                                       |
          +----------------------+              +----------------------+
          |      SQL Databases   |              |    NoSQL Databases   |
          |   (Relational DB)    |              |  (Non-Relational)   |
          +----------+-----------+              +----------+-----------+
                     |                                      |
          ---------------------------           ------------------------------------
          |           |             |           |        |        |              |
+----------------+ +----------------+ +----------------+ +-----------+ +-----------+ +-----------+
|   Structured   | | Fixed Schema   | |   ACID Model   | | Key-Value | | Document  | | Column /  |
|   Tables       | | (Predefined)   | | Transactions   | | Store     | | Store     | | Graph DB  |
+----------------+ +----------------+ +----------------+ +-----------+ +-----------+ +-----------+
                     |                                      |
               +-------------+                     +-------------------------+
               |  Examples   |                     |        Examples         |
               |  MySQL      |                     |  MongoDB, Redis         |
               |  PostgreSQL |                     +-------------------------+
               +-------------+


Choose SQL when â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ You need strong consistency (banking, inventory), complex queries with joins, well-defined schemas that rarely change, or ACID guarantees.

Choose NoSQL when â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ You need horizontal scalability, your schema evolves frequently, you are dealing with high write throughput, or your data is naturally unstructured (logs, social media posts).


â¤ The reality is that most production systems use both.
â¤ An e-commerce platform might store orders in PostgreSQL (needs ACID) and product catalog in MongoDB (flexible schema, read-heavy).
â¤ The key interview insight is explaining why you chose one over the other for a specific part of your system.

â“ Once you have data in a database, how do you find it quickly?


3ï¸âƒ£ Database Indexing
â¤ Without an index, finding a row in a table means scanning every single row, one by one.
â¤ That is fine for 100 rows. For 100 million rows? That is a disaster.
â¤ An index is a data structure that makes lookups fast, like the index at the back of a textbook.

Most databases use B-tree indexes by default.
â¤ A B-tree organizes data in a sorted, balanced tree structure that supports O(log n) lookups.
â¤ When you create an index on the email column, the database builds a separate B-tree that maps email values to their row locations on disk.

The trade-off: indexes speed up reads but slow down writes (every INSERT or UPDATE must also update the index).
â¤ They also consume storage space. You should index columns that appear in WHERE clauses and JOIN conditions, but avoid indexing everything.

â“ Indexes help you find data faster, but what happens when a single database cannot hold all your data?


4ï¸âƒ£ Vertical Partitioning
When your database tables grow wide with dozens of columns, vertical partitioning splits them into narrower tables. Each partition holds a subset of columns, and they are joined by a shared key.

                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚        EMPLOYEE TABLE          â”‚
                         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                         â”‚ emp_id | name | age | salary | â”‚
                         â”‚ address | phone | dept | DOJ | â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                   Vertical Partition â”‚ (Column-wise Split)
                                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        EMP_BASIC_INFO          â”‚     â”‚      EMP_CONFIDENTIAL_INFO     â”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
        â”‚ emp_id (PK)                    â”‚     â”‚ emp_id (PK)                    â”‚
        â”‚ name                           â”‚     â”‚ salary                         â”‚
        â”‚ age                            â”‚     â”‚ address                        â”‚
        â”‚ dept                           â”‚     â”‚ phone                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                                     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ JOIN ON emp_id â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ The idea is to separate data by access pattern. Your user profile page needs name and avatar, not the billing address.
â¤ By splitting the table, each query touches only the columns it actually needs, which means fewer bytes read from disk and faster responses.

â¤ Vertical partitioning also improves security (billing data can have stricter access controls) and allows each partition to be optimized independently (different indexes, different storage engines).
â¤ In interviews, mention it when you have a table with many columns that are accessed in different contexts.

â¤ Sometimes the problem is not wide tables but too many rows.
â¤ And sometimes the solution is not better querying, but smarter storage. What if you stored pre-computed results?


5ï¸âƒ£ Caching
â¤ Caching stores frequently accessed data in a fast layer (usually memory) so you do not have to hit the database for every request.
â¤ It is the single most effective technique for improving read performance.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client     â”‚
â”‚ (Browser /   â”‚
â”‚ Mobile App)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Request (GET /data)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API /      â”‚
â”‚ Application  â”‚
â”‚   Server     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1ï¸âƒ£ Check Cache
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cache      â”‚
â”‚ (Redis /     â”‚
â”‚ Memcached)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Cache Hit âœ…        Cache Miss âŒ
       â”‚                    â”‚
       â”‚                    â–¼
       â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚            â”‚  Database    â”‚
       â”‚            â”‚ (MySQL /     â”‚
       â”‚            â”‚ MongoDB)     â”‚
       â”‚            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â”‚      2ï¸âƒ£ Store Data â”‚
       â”‚         in Cache   â”‚
       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Response     â”‚
â”‚ Sent to      â”‚
â”‚ Client       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â¤ The pattern shown above is called cache-aside (or lazy loading).
â¤ The application checks the cache first. On a hit, it returns immediately.
â¤ On a miss, it fetches from the database, stores the result in the cache, and then returns it.
â¤ Redis and Memcached are the most popular in-memory caches.

â¤ The hard part of caching is invalidation: when the underlying data changes, the cache needs to be updated or cleared.
â¤ Common strategies include TTL (time-to-live, data expires automatically), write-through (update cache and database together), and write-behind (update cache first, database later).

â“ Caching helps with read performance. But what about queries that need to join multiple tables?


6ï¸âƒ£ Denormalization
â¤ In normalized databases, data is split across many tables to avoid duplication.
â¤ That is clean, but joining 5 tables to load a single page is slow. Denormalization deliberately adds redundant data to reduce the number of joins.

          NORMALIZED DATABASE (Before)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Users Table     â”‚
â”‚ user_id (PK)         â”‚
â”‚ name                 â”‚
â”‚ email                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ JOIN
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Orders Table    â”‚
â”‚ order_id (PK)        â”‚
â”‚ user_id (FK)         â”‚
â”‚ order_date           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ JOIN
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Order_Items Table â”‚
â”‚ item_id              â”‚
â”‚ order_id (FK)        â”‚
â”‚ product_id           â”‚
â”‚ quantity             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            âŒ Multiple JOINs
            âŒ Slower Reads
            âŒ Complex Queries

                   â†“ DENORMALIZATION â†“

        DENORMALIZED DATABASE (After)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Orders_Denormalized       â”‚
â”‚ order_id                         â”‚
â”‚ user_id                          â”‚
â”‚ user_name        â† duplicated    â”‚
â”‚ user_email       â† duplicated    â”‚
â”‚ order_date                       â”‚
â”‚ product_id                       â”‚
â”‚ quantity                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            âœ… No JOINs
            âœ… Faster Reads
            âŒ Data Duplication



â¤ In the normalized version, displaying an order with the user's name and product title requires joining three tables.
â¤ In the denormalized version, all the data is in one table, so a single read returns everything.
â¤ The trade-off: you are storing user_name and product_title in the orders table, which means if a user changes their name, you need to update it in multiple places.

â¤ Denormalization is a read optimization.
â¤ Use it when your system is read-heavy and those reads involve expensive joins.
â¤ It is very common in NoSQL databases, where joins are not natively supported.
â¤ In interviews, always pair denormalization with a strategy for keeping the redundant data consistent.

â“ Databases handle structured data. But what about images, videos, and large files?


7ï¸âƒ£ Blob Storage
â¤ Not all data fits neatly into rows and columns.
â¤ Images, videos, PDFs, backups, these are "binary large objects" (blobs) that need specialized storage.
â¤ Blob storage systems like Amazon S3 are designed to store massive amounts of unstructured data cheaply and reliably.

â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ User â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜
   â”‚ 1ï¸âƒ£ Upload File
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App Server   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ Store File
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Blob Store                   â”‚
â”‚ :contentReference[oaicite:0]{index=0} â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ 2ï¸âƒ£ Return URL
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App Server   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ 3ï¸âƒ£ Store URL
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 4ï¸âƒ£ Replicate
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CDN Edge Nodes â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 5ï¸âƒ£ Download via CDN URL
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ User â”‚
â””â”€â”€â”€â”€â”€â”€â”˜

â¤ The pattern is simple: store the file in blob storage, get back a URL, and store that URL in your database.
â¤ Your database stays lean (it holds a URL string, not a 10MB image), and the blob storage handles durability, replication, and serving.

â¤ Blob stores are optimized for large, immutable files.
â¤ They offer high durability (S3 promises 99.999999999% durability, also known as 11 nines), cheap storage, and built-in CDN integration for fast delivery.
â¤ In system design interviews, whenever you are dealing with media (images, videos, documents), always store them in blob storage and reference them by URL in your database.

â“ We have covered where data lives and how to access it efficiently. But what happens when your system grows beyond what a single server can handle?


============
ğŸ”· Scaling
=============
Your app is growing. A single server can not handle the load anymore. What now? Scaling is about handling more traffic, more data, and more users without things falling apart.

There are two fundamental approaches, and they are not mutually exclusive.

1ï¸âƒ£ Vertical Scaling (Scale Up)
â¤ The simplest way to handle more load: get a bigger machine.
â¤ More CPU, more RAM, faster disks. That is vertical scaling. No code changes required.

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Ceiling     â”‚
            â”‚ Hardware Limitâ”‚
            â”‚ Cannot scale  â”‚
            â”‚ further       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Upgrade
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Large Server        â”‚
        â”‚ 64 CPU, 256GB RAM       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Upgrade
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Medium Server        â”‚
        â”‚ 8 CPU, 32GB RAM         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Upgrade
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Small Server        â”‚
        â”‚ 2 CPU, 4GB RAM          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â¤ Vertical scaling is attractive because it is simple.
â¤ Your application does not need to worry about distributing work across multiple machines.
â¤ A single powerful server can handle a surprising amount of traffic, plenty for startups and small-to-medium applications.

â¤ The problem? There is a hard ceiling.
â¤ You can not keep buying bigger machines forever.
â¤ The largest cloud instances have limits, and they get disproportionately expensive.
â¤ You also have a single point of failure: if that one big server goes down, everything goes down.
â¤ That is why, beyond a certain point, you need a different approach.


2ï¸âƒ£ Horizontal Scaling (Scale Out)
â¤ Instead of making one server bigger, add more servers.
â¤ Horizontal scaling distributes the load across multiple machines, and there is no theoretical upper limit to how many you can add.

              Add more as needed
                    â–²
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Server N...         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Server 3           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Server 2           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Server 1           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–²
                    â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Load Balancer â”‚
             â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   Clients    â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ Horizontal scaling is how every large-scale system works.
â¤ Google, Netflix, Amazon, they all run thousands of servers behind load balancers.
â¤ Adding capacity is as simple as spinning up more instances.

â¤ The catch? Your application needs to be designed for it.
â¤ Servers should be stateless, meaning any server can handle any request without relying on local state.
â¤ Session data needs to live in a shared store (like Redis), not in server memory.
â¤ You also need a load balancer to distribute traffic, and your database needs its own scaling strategy (replication, sharding).
â¤ Horizontal scaling adds complexity, but it removes the ceiling.

â“ But how do you decide which server handles each request? That is where load balancers come in.


3ï¸âƒ£ Load Balancers
A load balancer sits between clients and your server pool, distributing incoming requests so no single server gets overwhelmed.\
It is the traffic cop of your system.

                Round Robin
             Least Connections
                    â–²
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Load Balancer       â”‚
        â”‚   (Health Check Fails)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Server 1    â”‚               â”‚  Server 2    â”‚
â”‚  Healthy     â”‚               â”‚  Healthy     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            âŒ Traffic Not Sent
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Server 3    â”‚
            â”‚  Down        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


        â–²           â–²           â–²
        â”‚           â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client 1  â”‚  â”‚  Client 2  â”‚  â”‚  Client 3  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â¤ Common algorithms include Round Robin (requests go to servers in order), Least Connections (send to the server handling the fewest requests), and Weighted (servers with more capacity get more traffic).
â¤ The load balancer also runs health checks: if a server stops responding, traffic is automatically routed to healthy servers.

â¤ Load balancers provide two critical benefits: scalability (distribute load across many servers) and availability (route around failures).
â¤ In system design, you will always place a load balancer in front of your application servers.
â¤ Some systems use multiple layers: one LB for web servers, another for application servers, another for databases.

â“ Load balancers distribute work across application servers. But what about the database? How do you scale reads?


4ï¸âƒ£ Replication
Replication copies your data across multiple database servers.
The most common setup is primary-replica: all writes go to the primary, and changes are replicated to one or more replicas that handle read queries.

                    Writes
                      â”‚
                      â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  App Server  â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Primary DB        â”‚
        â”‚     Read + Write       â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚          â”‚
        Replicates    Replicates
               â”‚          â”‚
               â–¼          â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Replica 1    â”‚   â”‚   Replica 2    â”‚
     â”‚   Read Only    â”‚   â”‚   Read Only    â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚
          Reads                 Reads
            â”‚                     â”‚
            â–¼                     â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  App Server  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ Most applications are read-heavy (think of how many times you scroll Twitter/X vs how many times you tweet).
â¤ By directing reads to replicas, you offload the primary database and can handle many times more read traffic.
â¤ If a replica fails, reads are redirected to other replicas.
â¤ If the primary fails, a replica can be promoted to take over.

â¤ The trade-off is replication lag: there is a small delay between when data is written to the primary and when it appears on replicas.
â¤ This means reads from replicas might return slightly stale data.
â¤ For most applications this is fine (your tweet appearing half a second later on someone else's feed is acceptable), but for critical operations like account balance checks, you should read from the primary.

â“ Replication handles read scaling. But what about write scaling, when a single primary cannot keep up with write volume?


5ï¸âƒ£ Sharding
â¤ Sharding splits your data across multiple database nodes, where each node holds a different subset of the data.
â¤ Unlike replication (every node has all the data), sharding means each node has only a portion.
â¤ This distributes both storage and write load.

                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  App Server  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Shard Router    â”‚
                   â”‚ shard_key % N    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â”‚     â”‚     â”‚
        user_id 1-1M      â”‚     â”‚     â”‚
             â–¼            â–¼     â–¼     â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Shard 1    â”‚ â”‚   Shard 2    â”‚ â”‚   Shard 3    â”‚
       â”‚ user_id      â”‚ â”‚ user_id      â”‚ â”‚ user_id      â”‚
       â”‚ 1â€“1M         â”‚ â”‚ 1Mâ€“2M        â”‚ â”‚ 2Mâ€“3M        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ The shard key determines which shard holds each piece of data.
â¤ A common approach is hash-based sharding: compute a hash of the key (like user_id) and use modulo to pick a shard.
â¤ Range-based sharding assigns consecutive key ranges to each shard.
â¤ The choice of shard key is critical, a bad key creates "hot shards" where one node gets most of the traffic.

â¤ Sharding challenges include: cross-shard queries (joining data across shards is expensive), rebalancing (adding a new shard means redistributing data), and operational complexity.

ğŸ‘‰ We have covered scaling individual components.But once you are running multiple servers and databases, a new set of challenges appears.


========================
ğŸ”· Distributed Systems
========================
â¤ Once you have multiple servers, databases, and caches spread across a network, you enter the world of distributed systems.
â¤ Things that are trivial on a single machine, like keeping data consistent, become genuinely hard when machines can fail independently and networks can partition.

1ï¸âƒ£ CAP Theorem
The CAP theorem states that a distributed system can only guarantee two out of three properties: Consistency (every read returns the latest write), Availability (every request gets a response), and Partition Tolerance (the system works even if network links between nodes fail).

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     CAP Theorem    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Consistency  â”‚     â”‚ Availability â”‚     â”‚ Partition        â”‚
â”‚ Latest data  â”‚     â”‚ Always       â”‚     â”‚ Tolerance        â”‚
â”‚ always       â”‚     â”‚ responds     â”‚     â”‚ Survives network â”‚
â”‚              â”‚     â”‚              â”‚     â”‚ splits           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                    â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  CP Systems  â”‚     â”‚  AP Systems  â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ MongoDB                       â”‚     â”‚ Cassandra              â”‚
      â”‚ HBase                         â”‚     â”‚ DynamoDB               â”‚
      â”‚ Redis Cluster                 â”‚     â”‚ CouchDB                â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ Since network partitions are unavoidable in distributed systems, the real choice is between CP (consistency + partition tolerance) and AP (availability + partition tolerance).

â¤ CP systems (like MongoDB, HBase) refuse to serve requests if they cannot guarantee the data is up-to-date.
â¤ Better for banking, inventory, or anything where stale data causes real problems.
â¤ AP systems (like Cassandra, DynamoDB) always respond, even if the data might be slightly stale.
â¤ Better for social media feeds, product catalogs, or analytics where availability matters more than perfect accuracy.

ğŸ‘‰ Data consistency is one challenge of distributed systems. Another is delivering content fast to users around the world.


2ï¸âƒ£ CDN (Content Delivery Network)
A CDN is a network of servers distributed across the globe that caches and serves content from locations close to users.
Instead of every request traveling to your origin server in Virginia, users get content from the nearest edge node.

                        Push Content
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Origin Server (Virginia)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚               â”‚               â”‚
         Push Content     Push Content     Push Content
                â”‚               â”‚               â”‚
                â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Node     â”‚   â”‚ Edge Node     â”‚   â”‚ Edge Node     â”‚
â”‚ London        â”‚   â”‚ Singapore     â”‚   â”‚ Mumbai        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â–¼                   â–¼                   â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ User UK â”‚         â”‚ User SEAâ”‚         â”‚ User Indiaâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ CDNs primarily serve static content: images, CSS, JavaScript, videos.
â¤ Some CDNs (like CloudFlare) also cache dynamic content or run serverless functions at the edge.
â¤ The first user in a region gets a cache miss (request goes to origin), but subsequent users in that region get the cached copy, which is orders of magnitude faster.

â¤ Major CDN providers include CloudFront (AWS), Cloudflare, and Akamai.
â¤ In system design interviews, always include a CDN when your system serves media or static assets.
â¤ It reduces latency, decreases load on your origin server, and improves availability (even if your origin goes down, cached content can still be served).

â“ CDNs handle content delivery. But what about ensuring that retried requests do not cause duplicate side effects?


3ï¸âƒ£ Idempotency
â¤ In distributed systems, requests can fail and be retried.
â¤ A network timeout does not mean the request failed, maybe the server processed it but the response got lost.
â¤ If the client retries, you could end up charging a customer twice or sending duplicate emails. Idempotency prevents this.

CLIENT                                  SERVER
â”‚                                       â”‚
â”‚ POST /payments                        â”‚
â”‚ Idempotency-Key: abc-123              â”‚
â”‚ amount: $50                           â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚
â”‚                                       â”‚
â”‚                                       â”‚ Process payment
â”‚                                       â”‚ Store key abc-123
â”‚                                       â”‚ payment_id: 789
â”‚                                       â”‚
â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 200 OK                               â”‚
â”‚ payment_id: 789                      â”‚
â”‚                                       â”‚
â”‚ Network issue, client retries...     â”‚
â”‚                                       â”‚
â”‚ POST /payments                        â”‚
â”‚ Idempotency-Key: abc-123              â”‚
â”‚ amount: $50                           â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚
â”‚                                       â”‚
â”‚                                       â”‚ Key abc-123 already seen
â”‚                                       â”‚ Return stored result
â”‚                                       â”‚
â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 200 OK                               â”‚
â”‚ payment_id: 789                      â”‚
â”‚ (same result, no duplicate charge)   â”‚
â”‚                                       â”‚


â¤ An idempotent operation produces the same result regardless of how many times you execute it.
â¤ GET and DELETE are naturally idempotent (getting a resource twice returns the same thing, deleting an already-deleted resource is a no-op).
â¤ POST is not naturally idempotent, which is why you add an idempotency key: a unique identifier the client sends with each request.
â¤ The server checks if it has already processed that key and returns the cached result instead of processing again.

â¤ Stripe, PayPal, and most payment APIs require idempotency keys for exactly this reason.
â¤ In system design interviews, mention idempotency whenever you discuss payments, order creation, or any operation where duplicates would be harmful.

â“ With all these building blocks, individual concepts, how do you actually organize a large-scale system?


===========================
ğŸ”· Architecture Patterns
============================
With all the building blocks in place, from networking to storage to scaling, the final question is: how do you actually organize all these pieces into a coherent system? Architecture patterns give you proven blueprints for structuring large applications.

1ï¸âƒ£ Microservices
As applications grow, a single codebase (monolith) becomes hard to maintain, deploy, and scale.
Microservices split the application into small, independent services, each responsible for one business capability, each with its own database.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Monolith                    â”‚
â”‚                                              â”‚
â”‚  Users + Orders + Payments + Inventory       â”‚
â”‚               (Single Codebase)               â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Single DB   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Users        â”‚      â”‚ Orders       â”‚
â”‚ Service      â”‚      â”‚ Service      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚
       â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Users DB     â”‚      â”‚ Orders DB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Payments     â”‚      â”‚ Inventory    â”‚
â”‚ Service      â”‚      â”‚ Service      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚
       â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Payments DB  â”‚      â”‚ Inventory DB â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ Each microservice can be developed, deployed, and scaled independently.
â¤ The payments service handling more traffic? Scale just that service. A bug in inventory? Fix and deploy it without touching anything else.
â¤ Different teams can own different services and even use different programming languages.

â¤ The downsides are real: distributed systems complexity (network calls instead of function calls), data consistency across services (no shared database means no easy joins), and operational overhead (monitoring, deploying, and debugging dozens of services).

â“ Speaking of async communication, how do services talk to each other without waiting?


2ï¸âƒ£ Message Queues
Synchronous communication (service A calls service B and waits for a response) creates tight coupling.
If service B is slow or down, service A suffers too. Message queues decouple services by introducing an intermediary that stores messages until the consumer is ready to process them.

PRODUCER / QUEUE SIDE                     CONSUMER SIDE
â”‚                                         â”‚
â”‚ 1ï¸âƒ£ Send message                         â”‚
â”‚                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚ â”‚ Producer            â”‚                  â”‚
â”‚ â”‚ Order Service       â”‚                  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚           â”‚                              â”‚
â”‚           â–¼                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Message Queue                      â”‚  â”‚
â”‚ â”‚ Kafka / RabbitMQ / SQS             â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚
â”‚           â”‚          â”‚          â”‚      â”‚
â”‚           â”‚          â”‚          â”‚      â”‚
â”‚           â–¼          â–¼          â–¼      â”‚
â”‚                                         â”‚
â”‚                         2ï¸âƒ£ Deliver when consumer ready
â”‚                                         â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                   â”‚ Consumer         â”‚  â”‚
â”‚                   â”‚ Email Service    â”‚  â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”‚                         2ï¸âƒ£ Deliver when consumer ready
â”‚                                         â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                   â”‚ Consumer         â”‚  â”‚
â”‚                   â”‚ Inventory Serviceâ”‚  â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”‚                         2ï¸âƒ£ Deliver when consumer ready
â”‚                                         â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                   â”‚ Consumer         â”‚  â”‚
â”‚                   â”‚ Analytics Serviceâ”‚  â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚



â¤ When a user places an order, the Order Service does not need to wait for the email to be sent, inventory to be updated, and analytics to be recorded.
â¤ It drops a message on the queue and moves on. Each downstream service picks up the message at its own pace.
â¤ If the email service is down for a minute, the messages wait in the queue and get processed when it comes back.

â¤ Message queues provide: decoupling (services do not need to know about each other), buffering (handle traffic spikes by absorbing bursts), reliability (messages persist even if consumers crash), and scalability (add more consumers to process faster).
â¤ Popular choices include Kafka (high throughput, event streaming), RabbitMQ (traditional message broker), and SQS (managed AWS service).

â“ Services are decoupled now, but every external client still needs to know the address of every service. How do you simplify that?


3ï¸âƒ£ Rate Limiting
When your API is public, or even when it is internal, you need to protect it from being overwhelmed. Rate limiting controls how many requests a client can make within a time window. It prevents abuse, protects backend services, and ensures fair usage.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ Request
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Rate Limiter     â”‚
â”‚   Check Counter    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚         â”‚
     â”‚         â”‚
     â–¼         â–¼
Under limit     Over limit
150/200 used    200/200 used
     â”‚         â”‚
     â”‚         â”‚
     â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Allow        â”‚   â”‚ Reject             â”‚
â”‚ Process Req. â”‚   â”‚ 429 Too Many Req.  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend      â”‚
â”‚ Server       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ Common algorithms include Token Bucket (tokens refill at a fixed rate; each request costs a token), Sliding Window (count requests in a rolling time window), and Fixed Window (count requests in discrete time intervals).
â¤ The rate limiter typically sits at the API gateway level and uses a fast store like Redis to track request counts per client.

â¤ Rate limiting is essential for: preventing DDoS attacks, protecting expensive operations (like database queries), enforcing API usage tiers (free users get 100 req/min, paid users get 10,000), and ensuring one bad client does not ruin the experience for everyone else.

â“ Rate limiting is one of many cross-cutting concerns. How do you manage all of them, authentication, routing, rate limiting, without duplicating logic across every service?


4ï¸âƒ£ API Gateway
â¤ An API gateway is a single entry point for all client requests.
â¤ Instead of clients talking directly to dozens of microservices, they talk to the gateway, which handles routing, authentication, rate limiting, and other cross-cutting concerns.

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Web Client   â”‚        â”‚ Mobile App   â”‚        â”‚ 3rd Party    â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                      â”‚                      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚       API Gateway        â”‚
                   â”‚ Auth, Rate Limit,        â”‚
                   â”‚ Route, Transform         â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚          â”‚
                           â”‚          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Users Service  â”‚        â”‚ Orders Service â”‚   â”‚Payments Serviceâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           |
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Search Service â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ Without an API gateway, every service would need to implement authentication, rate limiting, and logging independently.
â¤ The gateway centralizes these concerns.
â¤ It also simplifies the client: instead of knowing the addresses of 20 services, the client knows one URL.
â¤ The gateway routes each request to the right service based on the path.

â¤ Additional gateway capabilities include: request/response transformation (converting between protocols or data formats), response aggregation (combining results from multiple services into one response), caching, and circuit breaking (stopping requests to a failing service).
â¤ Popular options include Kong, AWS API Gateway, and Nginx.


===================================
ğŸ”· Core Concepts of System Design
===================================
1ï¸âƒ£ Scalability
â¤ Every successful application eventually hits the same wall: more users, more data, more requests.
â¤ What worked smoothly for 1,000 users starts showing cracks at 100,000.
â¤ What handled 100 requests per second crumbles at 10,000.
ğŸ‘‰ This is where scalability becomes critical.

â¤ Scalability is the ability of a system to handle increased load by adding resources.
â¤ The key word here is "ability", a scalable system can grow to meet demand without requiring a complete architectural overhaul.


ğŸ”„ Measuring Scalability
Before scaling, you need to understand how to measure it. You cannot improve what you do not measure, and vague statements like "we need to scale" are useless without concrete numbers.

Scalability is typically evaluated along these dimensions:

1ï¸âƒ£ Load Metrics
| Metric                    | Description                                       | Example             |
| ----------------------------- | ----------------------------------------------------- | ----------------------- |
| Requests per second (RPS) | Number of API calls the system handles per second     | 10,000 RPS              |
| Concurrent users          | Users active at the same time                         | 50,000 concurrent users |
| Data volume               | Total amount of data stored or processed              | 10 TB storage           |
| Throughput                | Data transferred per unit time                        | 1 GB/s                  |
| Query rate (QPS)          | Database queries executed per second                  | 50,000 QPS              |
| Message rate              | Messages processed via queues (Kafka, RabbitMQ, etc.) | 100,000 messages/sec    |


2ï¸âƒ£ Performance Under Load
A system scales well if it maintains acceptable performance as load increases. This is often measured by response time or latency.
| Load Increase | Response Time | Behavior | What It Indicates                            |
| ----------------- | ----------------- | ------------ | ------------------------------------------------ |
| 1Ã— (Baseline) | 50 ms             | Baseline     | Normal system operation                          |
| 2Ã—            | 55 ms             | Excellent    | Sublinear growth, caching & scaling working well |
| 5Ã—            | 70 ms             | Good         | System handling increased load efficiently       |
| 10Ã—           | 150 ms            | Acceptable   | Linear degradation, predictable scaling          |
| 10Ã—           | 500 ms            | Concerning   | Superlinear degradation, bottleneck forming      |
| 10Ã—           | Timeout           | Critical     | System has reached breaking point                |

â¤ The goal is to keep performance relatively stable as load increases.
â¤ Ideally, you want linear or sublinear degradation, where doubling load does not double response time.
â¤ When response times spike or the system starts timing out, you have hit a scalability wall.


ğŸ”„ Vertical Scaling (Scale Up)
â¤ Vertical scaling means adding more power to your existing machines.
â¤ Instead of adding more servers, you upgrade to bigger ones.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Server        â”‚ --> â”‚ UPGRADE  â”‚ --> â”‚         Server         â”‚
â”‚----------------------â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚------------------------â”‚
â”‚  CPU  : 4 Cores      â”‚                      â”‚  CPU  : 32 Cores       â”‚
â”‚  RAM  : 16 GB        â”‚                      â”‚  RAM  : 256 GB         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        BEFORE                                      AFTER


This is often the first response to performance problems because it requires no architectural changes.

ğŸ”„ Common Vertical Scaling Actions
1ï¸âƒ£ Add more CPU cores for compute-intensive workloads
2ï¸âƒ£ Increase RAM to cache more data in memory
3ï¸âƒ£ Use faster SSDs to reduce I/O bottlenecks
4ï¸âƒ£ Upgrade network cards for higher bandwidth


ğŸ”„ Pros
1ï¸âƒ£ Simple â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ No code changes required. Just move to a bigger machine.
2ï¸âƒ£ Lower latency â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ All data is local, no network hops.
3ï¸âƒ£ No distributed complexity â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ A single server means no network partitions, no data synchronization issues.


ğŸ”„ Cons
1ï¸âƒ£ Hardware limits â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ You cannot scale beyond the largest available machine. Even cloud providers have limits.
2ï¸âƒ£ Single point of failure â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ One server means one failure point. If it goes down, everything goes down.
3ï¸âƒ£ Cost curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Larger machines cost disproportionately more. Doubling capacity often more than doubles cost.
4ï¸âƒ£ Downtime during upgrades â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Migrating to a bigger machine typically requires downtime.


ğŸ”„ When to Use Vertical Scaling
Vertical scaling works well for:
1ï¸âƒ£ Databases where data locality matters (before sharding becomes necessary)
2ï¸âƒ£ Applications with strong consistency requirements
3ï¸âƒ£ Early-stage startups that need simplicity over scale
4ï¸âƒ£ Workloads with predictable, moderate growth

âš¡ Note:Never dismiss vertical scaling as "not scalable." Many real-world systems run on vertically scaled databases for years. The key is knowing when horizontal scaling becomes necessary.


ğŸ”„ Horizontal Scaling (Scale Out)
Vertical scaling eventually hits a ceiling. When the biggest available machine is not big enough, or when you need fault tolerance that a single machine cannot provide, you need a different approach.

Horizontal scaling means adding more machines rather than upgrading existing ones. Instead of one powerful server, you distribute the load across many commodity servers.

This is how companies like Google, Netflix, and Amazon handle billions of requests.

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Load Balancer â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚                   â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Server 1 â”‚       â”‚ Server 2 â”‚       â”‚ Server 3 â”‚  ...  â”‚ Server N â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Instead of one powerful server, you have many commodity servers working together.
A load balancer distributes incoming requests across all servers.


ğŸ”„ Pros
1ï¸âƒ£ No hard limit â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ You can keep adding servers as needed. Cloud providers make this nearly unlimited.
2ï¸âƒ£ Fault tolerance â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ If one server fails, others continue serving traffic. No single point of failure.
3ï¸âƒ£ Cost-effective â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Many smaller machines often cost less than one giant machine.
4ï¸âƒ£ Geographic distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ You can place servers closer to users for lower latency.


ğŸ”„ Cons
1ï¸âƒ£ Complexity â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Distributed systems are harder to build, debug, and maintain.
2ï¸âƒ£ Data consistency â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Keeping data synchronized across servers is challenging.
3ï¸âƒ£ Network overhead â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Communication between servers adds latency.
4ï¸âƒ£ Stateless requirement â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Application servers typically need to be stateless, which may require architectural changes.


===================================
ğŸ”· Stateless ğŸ†š Stateful Services
====================================
For horizontal scaling to work effectively, services should be stateless. A stateless service does not store any session data locally. Each request can be handled by any server.

The difference is significant for scaling:
1ï¸âƒ£ Stateless Services
User A
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Balancer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Server 1 â”‚ â”‚ Server 2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚           â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Redis   â”‚
        â”‚ Session  â”‚
        â”‚  Store   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


2ï¸âƒ£ Stateful Services
User A
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Balancer  â”‚
â”‚ (Sticky Sess.) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        âœ– Cannot Route Here
â”‚ Server 1 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Has User Aâ”‚                           â”‚
â”‚ Session  â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚ Server 2 â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ In the stateful model, once a user's session is stored on Server 1, all their requests must go to that same server.
â¤ This creates hotspots and makes it risky to remove servers.
â¤ In the stateless model, session data lives in a shared store like Redis, so any server can handle any request.
â¤ The load balancer has complete freedom to distribute traffic.

To make services stateless:
1ï¸âƒ£ Store session data in a shared cache (Redis, Memcached)
2ï¸âƒ£ Use tokens (JWT) instead of server-side sessions
3ï¸âƒ£ Store uploaded files in object storage (S3) instead of local disk


ğŸ”„ Scaling Different Components
A typical system is not monolithic. It has multiple components, each with different scaling characteristics and challenges. Understanding these differences is crucial because the scaling strategy that works for one tier often does not work for another.

1ï¸âƒ£ Application Tier
Application servers are usually the easiest to scale horizontally, provided they are stateless:

Clients
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Balancer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    â”‚               â”‚
   â–¼    â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App Server â”‚  â”‚ App Server â”‚  â”‚ App Server â”‚
â”‚     1      â”‚  â”‚     2      â”‚  â”‚     3      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚               â”‚               â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Redis Cacheâ”‚   â”‚  Database  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ‘‰ Key strategies:
1ï¸âƒ£ Make services stateless
2ï¸âƒ£ Use a load balancer to distribute traffic
3ï¸âƒ£ Auto-scale based on CPU, memory, or request count
4ï¸âƒ£ Deploy across multiple availability zones


ğŸ”„ Database Tier
â¤ Databases are typically the hardest to scale because they manage state.
â¤ Unlike application servers, you cannot simply spin up more database instances and put a load balancer in front of them.
â¤ Data consistency, durability, and transaction isolation all complicate matters.

The approach depends on your workload pattern:
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   What's your          â”‚
                         â”‚     bottleneck?        â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚            â”‚             â”‚               â”‚
        â–¼               â–¼            â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Read-heavy  â”‚  â”‚ Write-heavy  â”‚  â”‚        Both        â”‚  â”‚ Need flexibilityâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                   â”‚                         â”‚
       â–¼                 â–¼                   â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Read Replicasâ”‚  â”‚   Sharding   â”‚  â”‚ Sharding + Replicasâ”‚  â”‚  Consider NoSQL  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                   â”‚
       â–¼                 â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Typical 10:1â€“100:1â”‚ â”‚ High write volume or â”‚ â”‚ High read + high write   â”‚
â”‚ read:write ratio  â”‚ â”‚ large dataset        â”‚ â”‚ load at large scale      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ Read Replicas
For read-heavy workloads (which most applications are), create copies of your database that handle read queries:

                         Writes
                           â”‚
                           â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Primary DB  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                 Replication â”‚ Replication
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
              â–¼                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Read Replica â”‚           â”‚ Read Replica â”‚
        â”‚      1       â”‚           â”‚      2       â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                           â”‚
             Reads                       Reads
               â”‚                           â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ Application  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Primary handles all writes, replicas receive changes and serve reads.
When to use â¡ Read-to-write ratio is 10:1 or higher, and writes are not the bottleneck.


ğŸ”„ Pros
1ï¸âƒ£ Simple to set up (managed services handle it)
2ï¸âƒ£ Offloads read traffic from primary
3ï¸âƒ£ Provides read availability if primary fails
4ï¸âƒ£ No application changes for basic setup


ğŸ”„ Cons
1ï¸âƒ£ Does not help with write-heavy workloads
2ï¸âƒ£ Introduces replication lag (stale reads)
3ï¸âƒ£ Replicas consume storage (full data copy)
4ï¸âƒ£ Failover can cause brief inconsistency


ğŸ” Sharding (Partitioning)
When read replicas are not enough, or when write volume exceeds what a single primary can handle, you need to split your data across multiple databases based on a partition key:

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Application  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Shard Router â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                     â”‚                     â”‚
          â–¼                     â–¼                     â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Shard 1  â”‚          â”‚ Shard 2  â”‚          â”‚ Shard 3  â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                     â”‚                     â”‚
     Users Aâ€“H              Users Iâ€“P              Users Qâ€“Z


ğŸ”„ Pros
1ï¸âƒ£ Distributes both reads AND writes
2ï¸âƒ£ Scales horizontally (add more shards)
3ï¸âƒ£ Each shard is smaller, faster
4ï¸âƒ£ Can place shards in different regions


ğŸ”„ Cons
1ï¸âƒ£ Complex to implement correctly
2ï¸âƒ£ Cross-shard queries are expensive or impossible
3ï¸âƒ£ Rebalancing shards is operationally difficult
4ï¸âƒ£ Transactions across shards are very hard


ğŸ”„ Common sharding strategies:
1ï¸âƒ£ Range-based â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Shard by value ranges (A-H, I-P, Q-Z)
2ï¸âƒ£ Hash-based â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Hash the key and mod by number of shards
3ï¸âƒ£ Directory-based â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Maintain a lookup table mapping keys to shards


ğŸ” NoSQL Databases
NoSQL databases like Cassandra, MongoDB, and DynamoDB are designed for horizontal scaling from the ground up:
1ï¸âƒ£ Built-in sharding â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Data is automatically distributed
2ï¸âƒ£ Eventual consistency â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Trade strong consistency for availability
3ï¸âƒ£ No joins â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Data model must accommodate denormalization


ğŸ”„ Pros
1ï¸âƒ£ Built-in sharding (automatic distribution)
2ï¸âƒ£ Designed for horizontal scale
3ï¸âƒ£ Often better write performance
4ï¸âƒ£ Schema flexibility


ğŸ”„ Cons
1ï¸âƒ£ Different query patterns than SQL
2ï¸âƒ£ No joins (denormalization required)
3ï¸âƒ£ Eventual consistency in many cases
4ï¸âƒ£ Less tooling ecosystem than SQL


ğŸ”„ Caching Tier
â¤ Caching reduces load on databases and improves response times.
â¤ A well-designed cache can handle 100x the throughput of a database, making it essential for high-traffic systems.
â¤ Redis, for example, can handle 100,000+ operations per second on a single node.

                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Application  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                             Miss
                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Redis Node 1 â”‚       â”‚ Redis Node 2 â”‚       â”‚ Redis Node 3 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         Miss                   Miss                   Miss
           â”‚                      â”‚                      â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Database   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”„ Cache Scaling strategies:
1ï¸âƒ£ Redis Cluster â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Automatically partitions data across nodes using hash slots
2ï¸âƒ£ Consistent hashing â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Distributes keys evenly and minimizes redistribution when nodes are added or removed
3ï¸âƒ£ Cache-aside pattern â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Application checks cache first, falls back to database on cache miss, then populates the cache


ğŸ” Message Queue Tier
â¤ Message queues are essential for scaling asynchronous workloads.
â¤ They decouple producers from consumers, allowing each to scale independently, and they buffer traffic spikes so consumers can process at their own pace.

        Producer 1                Producer 2
            â”‚                         â”‚
            â”‚                         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚     Message Queue    â”‚
             â”‚   Kafka / RabbitMQ   â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
   Consumer 1       Consumer 2       Consumer 3

â“ How queues help scalability:
1ï¸âƒ£ Decouple producers and consumers â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Scale each independently
2ï¸âƒ£ Buffer traffic spikes â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Queue absorbs bursts, consumers process at their own pace
3ï¸âƒ£ Partition topics â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Kafka partitions allow parallel consumption


âœ… Example: Scaling from 0 to millions of users
Theory is useful, but seeing scalability in action makes it stick. Let us walk through how a startup might scale a social media application from zero to millions of users. Each stage solves a specific bottleneck that emerged from growth.

          Users
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Single Server â”‚
     â”‚                â”‚
     â”‚  Application   â”‚
     â”‚     +          â”‚
     â”‚    MySQL       â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â¤ At launch, everything runs on one machine.
â¤ The application and database share the same server.
â¤ This setup is simple, cheap, and perfectly adequate for a few thousand users.
â¤ There is no distributed system complexity, no network latency between components, and debugging is straightforward.

â¤ The bottleneck emerges when the application and database start competing for CPU and memory on the same machine.


Stage 2ï¸âƒ£: Separate Database (10K-100K users)
        Users
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  App Server  â”‚
   â”‚ Application  â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚        MySQL         â”‚
   â”‚   Dedicated Server   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â¤ The first scaling move is usually separating the database onto its own machine.
â¤ Now each component can be tuned independently.
â¤ You can give the database server more RAM for caching, while the app server gets more CPU for request processing.

â¤ The bottleneck shifts to the database.
â¤ As user counts grow, the database handles more queries, and read operations start slowing down.


Stage 3ï¸âƒ£: Add Caching (100K-500K users)

Users
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Cache Miss   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Cache Miss   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App Server â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Redis Cacheâ”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   MySQL    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â¤ Adding a cache layer dramatically reduces database load.
â¤ Hot data, things like user profiles, recent posts, and session data, gets served from memory.
â¤ Redis can handle hundreds of thousands of reads per second, far more than MySQL.
â¤ With a good caching strategy, 80-90% of reads never hit the database.

â¤ The bottleneck is now the single app server.
â¤ It cannot handle the incoming request volume.


Stage 4ï¸âƒ£: Multiple App Servers (500K-2M users)

Users
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Balancerâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚App Server 1â”‚   â”‚App Server 2â”‚   â”‚App Server Nâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Redis  â”‚        â”‚ MySQL  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¤ This is where horizontal scaling begins.
â¤ A load balancer distributes traffic across multiple app servers.
â¤ Each server is stateless, storing no session data locally.
â¤ The Redis cache serves as the shared session store.

â¤ Adding more app servers is now trivial.
â“ Need more capacity? Spin up another server. Traffic spike during peak hours? Auto-scaling adds servers automatically.

â¤ The bottleneck shifts back to the database.
â¤ With more app servers generating more queries, the single MySQL instance becomes overwhelmed.


Stage 5ï¸âƒ£: Read Replicas (2M-10M users)

                         Writes
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App Serversâ”‚ â”€â”€â”€â”€â”€â–¶ â”‚ Primary MySQLâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚
       â”‚                 Replication
       â”‚                     â”‚
       â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚          â”‚                     â”‚
       â–¼          â–¼                     â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Redis  â”‚  â”‚ Replica 1â”‚        â”‚ Replica 2â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                      â”‚                   â”‚
                    Reads               Reads
                      â”‚                   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                             App Servers



â¤ Most applications are read-heavy, with reads outnumbering writes by 10:1 or more.
â¤ Read replicas take advantage of this pattern.
â¤ The primary database handles all writes, while replicas serve read queries.
â¤ This multiplies read capacity without changing the application much.

â¤ The trade-off is replication lag.
â¤ Replicas may be a few milliseconds behind the primary, so recently written data might not be immediately visible on reads.
â¤ For most applications, this is acceptable.
â¤ The bottleneck becomes write throughput.
â¤ One primary database can only handle so many writes per second.


Stage 6ï¸âƒ£: Sharding (10M+ users)

App Servers
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shard Router â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       â”‚               â”‚                â”‚
â–¼       â–¼               â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shard 1  â”‚      â”‚ Shard 2  â”‚      â”‚ Shard 3  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                 â”‚
Users 0â€“999999   Users 1Mâ€“1.99M       Users 2M+


â¤ Sharding is the final frontier of relational database scaling.
â¤ Data is partitioned across multiple databases based on a shard key, typically user ID.
â¤ Each shard handles a subset of users, distributing both read and write load.

â¤ This is powerful but comes with significant complexity.
â¤ Cross-shard queries become expensive or impossible.
â¤ Rebalancing shards when they grow unevenly is operationally challenging.
â¤ Many teams at this stage consider moving to distributed databases like CockroachDB or Vitess that handle sharding automatically.


==================
ğŸ”· Availability
==================
What good is a system that can handle millions of requests if it crashes when a single server fails?
This is where availability comes in.

â¤ Availability measures how often your system is operational and accessible to users.
â¤ A highly available system continues functioning even when individual components fail.
â¤ availability is not the same as reliability. A system can be highly available (always up) but unreliable (sometimes gives wrong answers).


ğŸ”„ Measuring Availability
Availability is typically expressed as a percentage of uptime over a given period. The formula is straightforward: Availability = Uptime / (Uptime + Downtime)

âœ… For example, if a system was up for 364 days and down for 1 day in a year: Availability = 364 / 365 = 99.73%

â¤ That single day of downtime drops you below "three nines" availability.
â¤ When you frame it as "one day," it sounds acceptable.


ğŸ”„ The "Nines" of Availability
Availability is often described in terms of "nines." Each additional nine dramatically reduces allowed downtime:
| Availability         | Downtime per Year | Downtime per Month | Downtime per Week |
| ------------------------ | --------------------- | ---------------------- | --------------------- |
| 99% (two nines)      | 3.65 days             | 7.3 hours              | 1.68 hours            |
| 99.9% (three nines)  | 8.76 hours            | 43.8 minutes           | 10.1 minutes          |
| 99.99% (four nines)  | 52.6 minutes          | 4.38 minutes           | 1.01 minutes          |
| 99.999% (five nines) | 5.26 minutes          | 26.3 seconds           | 6.05 seconds          |
| 99.9999% (six nines) | 31.5 seconds          | 2.63 seconds           | 0.6 seconds           |


ğŸ”„ Availability in Series vs Parallel
How you combine components dramatically affects overall availability.

1ï¸âƒ£ Components in Series
When components are in series, meaning all must work for the system to function, availability multiplies:
+-------------+     +-------------+     +-------------+
|  Web Server | --> | App Server  | --> |  Database   |
|   99.9%     |     |   99.9%     |     |   99.9%     |
+-------------+     +-------------+     +-------------+

Overall = 99.9% Ã— 99.9% Ã— 99.9% = 99.7%
â¤ Each component in the chain reduces overall availability.
â¤ You started with three components, each at "three nines," but the combined system is below three nines.
â¤ Add more components in series, and availability keeps dropping.


2ï¸âƒ£ Components in Parallel
When components are in parallel, meaning any can handle the request, availability improves dramatically:
                 +----------------+
                 |  Load Balancer |
                 +----------------+
                         |
            --------------------------------
            |                              |
     +---------------+            +---------------+
     |   Server 1    |            |   Server 2    |
     |    99.9%      |            |    99.9%      |
     +---------------+            +---------------+
For both servers to be down simultaneously, both must fail at the same time:
       âœ”ï¸ Failure probability = 0.1% Ã— 0.1% = 0.0001%
       âœ”ï¸ Availability = 100% - 0.0001% = 99.9999%
Two servers with 99.9% availability each give you nearly six nines when running in parallel. This is the power of redundancy.


ğŸ”„ Common Failure Modes
â¤ To design for availability, you must understand how things fail.
â¤ Failures do not ask permission, and they rarely happen at convenient times.
â¤ Knowing the common failure modes helps you prepare for them.

1ï¸âƒ£ Hardware Failures
Everything physical eventually breaks. The question is when, not if.
| Component      | Typical Failure Rate | MTBF (Mean Time Between Failures) |
| ------------------ | ------------------------ | ------------------------------------- |
| Hard Drive (HDD)   | 2â€“4% per year            | ~300,000 hours                        |
| SSD                | 0.5â€“1% per year          | ~1â€“2 million hours                    |
| Server             | 2â€“4% per year            | ~300,000 hours                        |
| Network Switch     | 1â€“2% per year            | ~500,000 hours                        |
| Power Supply (PSU) | 1â€“3% per year            | ~400,000 hours                        |
MTBF = Mean Time Between Failures.

â¤ At scale, hardware failures are not exceptional events.
â¤ They are routine. A data center with 10,000 servers will see hundreds of hardware failures per year.
â¤ If your architecture cannot handle a server dying at any moment, you do not have a highly available system.


2ï¸âƒ£ Software Failures
Hardware breaks randomly. Software breaks creatively.
1ï¸âƒ£ Bugs â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Code defects that cause crashes or incorrect behavior
2ï¸âƒ£ Memory leaks â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Gradual resource exhaustion
3ï¸âƒ£ Deadlocks â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Processes waiting on each other indefinitely
4ï¸âƒ£ Cascading failures â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ One failure triggering failures in dependent systems


3ï¸âƒ£ Network Failures
Networks fail in ways that are subtle, intermittent, and painful to debug.
1ï¸âƒ£ Packet loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Data does not reach its destination
2ï¸âƒ£ Latency spikes â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Delays in communication
3ï¸âƒ£ Partition â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Network split isolates groups of servers
4ï¸âƒ£ DNS failures â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Name resolution stops working


4ï¸âƒ£ Human Errors
Hereâ€™s the uncomfortable reality: many studies attribute 70â€“80% of outages to human error, not hardware, not software, but people.

âœ… Common examples:
1ï¸âƒ£ Configuration mistakes â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ wrong environment variable, typo in a config file
2ï¸âƒ£ Failed deployments â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ bad code or broken migrations pushed to production
3ï¸âƒ£ Accidental deletions â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ running the wrong command in the wrong place
4ï¸âƒ£ Capacity planning errors â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ underestimating traffic for a launch
This is why automation, testing, and guardrails matter so much. Humans make mistakes. Good systems make those mistakes hard to make and easy to recover from.


ğŸ” Redundancy: The Foundation of Availability
â¤ If there is one concept that underpins all of availability, it is redundancy.
â¤ The logic is simple: if you have only one of something, when it fails, you have zero.
â¤ If you have two, when one fails, you still have one.

Redundancy means having backup components that can take over when primary components fail.

1ï¸âƒ£ Active-Passive (Standby)
â¤ In an active-passive configuration, one component handles all the work while another waits idle as a backup.
â¤ When the active component fails, the passive one takes over.

                     Traffic
                        |
                        v
                +----------------+
                |  Load Balancer |
                | / Failover Mgr |
                +----------------+
                        |
              -------------------------
              |                       |
      +-------------------+   +--------------------+
      |  Primary Server   |   |  Standby Server    |
      |      ACTIVE       |   |     PASSIVE        |
      +-------------------+   +--------------------+
              |
        (Serves Requests)

      âŒ If Primary Fails
              â†“
      âœ… Standby Becomes ACTIVE

Active-passive mode is commonly used in situations where you want a single source of truth and controlled writes like databases, stateful services, and systems requiring a single leader.


ğŸ”„ Pros
1ï¸âƒ£ Simple to reason about
2ï¸âƒ£ Standby typically uses fewer resources
3ï¸âƒ£ Clear source of truth


ğŸ”„ Cons
1ï¸âƒ£ Failover takes time (detection + promotion + routing changes)
2ï¸âƒ£ Standby may not be truly â€œproduction-readyâ€ because it isnâ€™t tested under real load
3ï¸âƒ£ Potential for split-brain problem


The standby can be configured in different states of readiness:
| Standby Type | State                                  | Failover Time  | Cost |
| ---------------- | ------------------------------------------ | ------------------ | -------- |
| Cold Standby | Powered off, needs to boot                 | Minutes            | Lowest   |
| Warm Standby | Running but not receiving traffic          | Seconds to minutes | Medium   |
| Hot Standby  | Running, data synchronized, ready to serve | Seconds            | Highest  |


1ï¸âƒ£ Cold Standby
â¤ Cold standby is cheapest but slowest.
â¤ The backup server is not running, so failover requires booting the machine, starting services, and potentially restoring data.
â¤ This might take 5-15 minutes, which is too slow for most production systems but acceptable for disaster recovery.


2ï¸âƒ£ Warm Standby
â¤ Warm standby keeps the backup running and configured, but not actively processing requests.
â¤ It might be receiving replicated data but is not in the load balancer pool.
â¤ Failover involves adding it to the pool and possibly promoting it, which takes seconds to a few minutes.


3ï¸âƒ£ Hot Standby
â¤ Hot standby is the most expensive but fastest.
â¤ The backup is fully synchronized and ready to serve immediately.
â¤ For databases, this often means synchronous replication where every write is confirmed on both primary and standby before acknowledging the client.


2ï¸âƒ£ Active-Active
â¤ In an active-active configuration, all components handle traffic simultaneously.
â¤ There is no distinction between primary and backup because every node is doing real work.

                     Traffic
                        |
                        v
                +----------------+
                |  Load Balancer |
                +----------------+
                        |
        ------------------------------------------------
        |                    |                         |
+-------------------+ +-------------------+ +-------------------+
|    Server 1       | |    Server 2       | |    Server 3       |
|     ACTIVE        | |     ACTIVE        | |     ACTIVE        |
+-------------------+ +-------------------+ +-------------------+

â¤ When one node fails, the load balancer simply stops sending traffic to it.
â¤ There is no failover process because the other nodes were already handling traffic.
â¤ The remaining nodes absorb the additional load.


ğŸ”„ Pros
1ï¸âƒ£ No failover delay
2ï¸âƒ£ All nodes tested under real load
3ï¸âƒ£ Better resource utilization


ğŸ”„ Cons
1ï¸âƒ£ More complex
2ï¸âƒ£ Must handle data consistency across nodes
3ï¸âƒ£ equires stateless design or shared state

â¤ The key requirement for active-active is that requests can be handled by any node.
â¤ This works naturally for stateless services where each request is independent.
â¤ For stateful services, you need either shared storage (like a database or Redis) or sticky sessions (which reduces availability benefits).


3ï¸âƒ£ Geographic Redundancy
Redundancy within a single data center protects against hardware failures, but what if the entire data center goes offline? Power outages, network cuts, natural disasters, or even a backhoe cutting a fiber line can take down an entire facility.

Geographic redundancy distributes your system across multiple physical locations:

                                   +---------+
                                   |  Users  |
                                   +---------+
                                        |
                                        v
                +-------------------------------------------+
                |   Global DNS (Route53 / CloudFlare)       |
                +-------------------------------------------+
                      |                 |                |
                   US East            US West           Europe
                      |                 |                |
   --------------------------------------------------------------------------------
   |                              |                               |               |
   |  +----------------------+    |  +----------------------+    |  +----------------------+
   |  | Availability Zone 1  |    |  | Availability Zone 2  |    |  | Availability Zone 3  |
   |  |     US East          |    |  |     US West          |    |  |      Europe          |
   |  |                      |    |  |                      |    |  |                      |
   |  |  +--------------+   |    |  |  +--------------+   |    |  |  +--------------+   |
   |  |  | Load Balancer|   |    |  |  | Load Balancer|   |    |  |  | Load Balancer|   |
   |  |  +--------------+   |    |  |  +--------------+   |    |  |  +--------------+   |
   |  |          |           |    |  |          |           |    |  |          |           |
   |  |          v           |    |  |          v           |    |  |          v           |
   |  |   +--------------+  |    |  |   +--------------+  |    |  |   +--------------+  |
   |  |   | App Servers  |  |    |  |   | App Servers  |  |    |  |   | App Servers  |  |
   |  |   +--------------+  |    |  |   +--------------+  |    |  |   +--------------+  |
   |  |          |           |    |  |          |           |    |  |          |           |
   |  |          v           |    |  |          v           |    |  |          v           |
   |  |     +-----------+   |    |  |     +-----------+   |    |  |     +-----------+   |
   |  |     | Database  |   |    |  |     | Database  |   |    |  |     | Database  |   |
   |  |     +-----------+   |    |  |     +-----------+   |    |  |     +-----------+   |
   |  |          |           |    |  |          |           |    |  |          |           |
   |  |          +-----------+----+--+----------+-----------+----+--+----------+           |
   |  |                 Cross-Region Database Replication                                  |
   |  ---------------------------------------------------------------------------------------

Cloud providers offer different levels of geographic redundancy:
| Level                    | What It Is                                                               | Protects Against                    | Latency Impact         |
| ---------------------------- | ---------------------------------------------------------------------------- | --------------------------------------- | -------------------------- |
| Availability Zones (AZs) | Separate data centers within the same region, connected by low-latency links | Single data center failure              | Minimal (â‰ˆ 1â€“2 ms)         |
| Regions                  | Geographically separate areas (e.g., US-East vs US-West)                     | Regional disasters, large-scale outages | Significant (â‰ˆ 50â€“100 ms+) |
| Multi-Cloud              | Infrastructure spread across different cloud providers (e.g., AWS + GCP)     | Cloud provider-wide outages             | Variable                   |


â¤ Availability Zones are the sweet spot for most applications.
â¤ They provide meaningful isolation (separate power, cooling, and network) while keeping latency low enough for synchronous replication.
â¤ Most cloud-native applications deploy across at least two AZs.


â¤ Multi-region deployment is necessary for global applications or those requiring disaster recovery from regional events.
â¤ The challenge is data replication, since synchronous replication across regions adds significant latency.
â¤ Most multi-region systems use asynchronous replication and accept some data loss in a disaster (typically seconds to minutes of transactions)


ğŸ”„ Redundancy Across Layers
â¤ A chain is only as strong as its weakest link.
â¤ If you have redundant app servers but a single database, the database is your single point of failure.
â¤ True high availability requires redundancy at every layer of your stack.

+-------------------+            +-------------------+
|      User 1       |            |      User 2       |
+-------------------+            +-------------------+
          |                                 |
          v                                 v
--------------------------------------------------------------------------------
|                                    DNS                                      |
|                                                                              |
|        +-------------------+                 +-------------------+           |
|        |   DNS Server 1    |                 |   DNS Server 2    |           |
|        +-------------------+                 +-------------------+           |
--------------------------------------------------------------------------------
          |                                 |
          v                                 v
--------------------------------------------------------------------------------
|                               Load Balancers                                 |
|                                                                              |
|        +-------------------+                 +-------------------+           |
|        | Load Balancer 1   |                 | Load Balancer 2   |           |
|        +-------------------+                 +-------------------+           |
--------------------------------------------------------------------------------
             |              \                     /              |
             |               \                   /               |
             v                v                 v                v
--------------------------------------------------------------------------------
|                                 App Servers                                  |
|                                                                              |
|     +-----------+          +-----------+          +-----------+              |
|     |   App 1   |          |   App 2   |          |   App 3   |              |
|     +-----------+          +-----------+          +-----------+              |
--------------------------------------------------------------------------------
              \                  |                  /
               \                 |                 /
                v                v                v
--------------------------------------------------------------------------------
|                                   Database                                   |
|                                                                              |
|                        +-------------------------+                           |
|                        |       Primary DB        |                           |
|                        +-------------------------+                           |
|                                     |                                        |
|                                     v                                        |
|                        +-------------------------+                           |
|                        |       Replica DB        |                           |
|                        +-------------------------+                           |
--------------------------------------------------------------------------------

ğŸ‘‰ Notice that redundancy gets harder as you move down the stack.
ğŸ‘‰ Adding more web servers is trivial.
ğŸ‘‰ Adding database replicas with automatic failover requires careful engineering.


âš¡ Note: Redundancy is not free. Every backup server, every replica, every additional availability zone costs money. The question is whether that cost is justified by the reduction in downtime risk.


ğŸ”„ High Availability Patterns
Patterns are reusable solutions to common problems. The following patterns appear repeatedly in highly available systems.

Pattern 1ï¸âƒ£: Load Balancer with Multiple Backends
â¤ The most common and fundamental pattern for stateless services.
â¤ A load balancer distributes traffic across multiple servers, automatically routing around failures.

                +---------+
                |  Users  |
                +---------+
                     |
                     v
              +----------------+
              | Load Balancer  |
              +----------------+
                 |      |      |
        Health Check   Health Check   Health Check
                 |      |      |
              +------+ +------+ +------+
              |Srv 1 | |Srv 2 | |Srv 3 |
              +------+ +------+ +------+


â“ How it provides high availability:
âœ”ï¸ Load balancer continuously monitors backend health
âœ”ï¸ Failed servers are automatically removed from rotation
âœ”ï¸ Traffic redistributes to healthy servers within seconds
âœ”ï¸ New servers can be added without any downtime


ğŸ”„ Load Balancer Redundancy
The load balancer itself is a single point of failure. For true high availability, you need redundant load balancers:

                   +---------+
                   |  Users  |
                   +---------+
                        |
                        v
                   +---------+
                   |   DNS   |
                   +---------+
                        |
                        v
                  +----------------+
                  |   Virtual IP   |
                  +----------------+
                        |
          -----------------------------------
          |                                 |
+----------------------+       +----------------------+
|  Load Balancer 1     |       |  Load Balancer 2     |
|      ACTIVE          |       |      STANDBY         |
+----------------------+       +----------------------+
           |
           v
    -------------------------
    |                       |
+-----------+         +-----------+
| Server 1  |         | Server 2  |
+-----------+         +-----------+

â¤ Cloud providers handle this automatically. AWS ALB, Google Cloud Load Balancer, and Azure Load Balancer are all managed services with built-in redundancy.
â¤ On-premises, you might use keepalived with a virtual IP that floats between two HAProxy instances.


Pattern 2ï¸âƒ£: Database Replication with Automatic Failover
Databases are stateful and cannot simply be load-balanced like web servers. Database high availability requires replication and careful failover management.

                               +------------------+
                               |   Application    |
                               +------------------+
                                         |
                                         v
                         +--------------------------------+
                         |          Primary DB            |
                         |        (Reads + Writes)        |
                         +--------------------------------+
                              |            |            |
                     Sync Replication   Async Replication   Async Replication
                              |            |            |
                              v            v            v
              +-------------------------+  +----------------------+  +----------------------+
              |     Sync Replica        |  |     Async Replica    |  |     Async Replica    |
              |   (Failover Target)     |  |    (Read Scaling)    |  |     (Analytics)      |
              +-------------------------+  +----------------------+  +----------------------+

                                         â–²
                                         |
                          +--------------------------------+
                          |        Failover Monitor        |
                          |   (Patroni / Orchestrator)    |
                          +--------------------------------+
                          |  - Health checks               |
                          |  - Leader election             |
                          |  - Automatic failover          |
                          +--------------------------------+


Synchronous vs Asynchronous Replication
| Type             | How It Works                                         | Data Loss                 | Performance Impact             |
| -------------------- | -------------------------------------------------------- | ----------------------------- | ---------------------------------- |
| Synchronous      | Write is confirmed only after replica acknowledges       | Zero (RPO = 0)                | Higher latency (waits for replica) |
| Asynchronous     | Write is confirmed immediately; replica catches up later | Possible (seconds to minutes) | No impact on write latency         |
| Semi-synchronous | Waits for at least one replica; others are async         | Minimal                       | Moderate impact                    |


â¤ Synchronous replication guarantees zero data loss but adds latency.
â¤ Every write must wait for the replica to confirm.
â¤ If your replica is in a different region, this adds significant latency (50-100ms per write).

â¤ Asynchronous replication has no performance impact but can lose data.
â¤ If the primary fails, any writes not yet replicated are lost.
â¤ The "replication lag" is typically seconds but can grow during high load.

Most production systems use synchronous replication for the failover target and asynchronous replication for read replicas and analytics.


Pattern 3ï¸âƒ£: Queue-Based Load Leveling
When downstream services cannot handle peak load, use a queue to buffer requests and process them at a sustainable rate.

+-------------------+
|   Web Servers     |
+-------------------+
          |
          v
+--------------------------------+
|        Message Queue           |
|   (Kafka / Amazon SQS)         |
+--------------------------------+
      |            |            |
      v            v            v
+-----------+  +-----------+  +-----------+
|  Worker 1 |  |  Worker 2 |  |  Worker 3 |
+-----------+  +-----------+  +-----------+
      \            |            /
       \           |           /
        v          v          v
               +-----------+
               | Database  |
               +-----------+


â“ How it provides high availability:
âœ”ï¸ Decouples producers from consumers
âœ”ï¸ Buffers traffic spikes that would overwhelm the database
âœ”ï¸ Workers can fail and restart without losing messages
âœ”ï¸ Can scale workers independently based on queue depth

This pattern is essential for handling bursty traffic. A flash sale might generate 100x normal traffic for a few minutes. Without a queue, the database would be overwhelmed. With a queue, orders accumulate and are processed at a sustainable rate.


Pattern 4ï¸âƒ£: Circuit Breaker
â¤ When a dependency fails, continuing to call it wastes resources and can cause cascading failures.
â¤ The circuit breaker pattern prevents this by failing fast.

                +----------------------+
                |        Start         |
                +----------------------+
                           |
                           v
                +----------------------+
                |        Closed        |
                |   Normal operation  |
                | Requests pass       |
                | through             |
                +----------------------+
                           |
           Failures exceed threshold
                           |
                           v
                +----------------------+
                |         Open         |
                |     Failing fast     |
                | Requests rejected   |
                | immediately         |
                +----------------------+
                           |
                    Timeout expires
                           |
                           v
                +----------------------+
                |       Half-Open      |
                |   Testing recovery  |
                | Limited requests    |
                | allowed             |
                +----------------------+
                    |             |
      Test request succeeds   Test request fails
                    |             |
                    v             v
        +------------------+   +----------------------+
        |      Closed      |   |         Open         |
        | Normal operation |   |   Failing fast       |
        | Requests pass    |   | Requests rejected    |
        | through          |   | immediately          |
        +------------------+   +----------------------+


| State     | Behavior                                                      | Transitions                                               |
| ------------- | ----------------------------------------------------------------- | ------------------------------------------------------------- |
| Closed    | Normal operation. All requests pass through. Tracks failure rate. | â†’ Open: when failure rate exceeds threshold               |
| Open      | Fail fast. All requests are immediately rejected with error.      | â†’ Half-Open: after timeout period                         |
| Half-Open | Testing state. Allows limited requests through.                   | â†’ Closed: if test succeeds <br> â†’ Open: if test fails |

â¤ A highly available system stays up even when components fail. But availability alone is not enough.
â¤ A system that is always up but sometimes gives wrong answers is not trustworthy.


================
ğŸ”· Reliability
=================
â¤ A reliable system performs its intended function correctly and consistently, even in the face of faults.
â¤ While availability asks "Is the system up?", reliability asks "Is the system doing what it should?"

âœ… For Example: The distinction matters. Consider a payment system that is always available but occasionally charges customers twice. Or a messaging app that delivers messages out of order.
ğŸ‘‰ These systems are available, but they are not reliable. And unreliable systems destroy user trust faster than unavailable ones.

â¤ Reliability is the probability that a system will perform its intended function correctly over a given period of time, under specified conditions.

This definition has several important parts.
1ï¸âƒ£ "Correctly" means producing the right output, not just any output.
2ï¸âƒ£ "Over a given period" means reliability is measured over time, not at a single instant.
3ï¸âƒ£ "Under specified conditions" means we define what normal operation looks like.
ğŸ‘‰ An available system responds. A reliable system responds correctly. You want both, but they are distinct properties.


ğŸ”„ Reliability vs Related Concepts
People often confuse reliability with availability and fault tolerance. Here is how they differ:

| Concept         | Question It Answers                    | Example                          |
| ------------------- | ------------------------------------------ | ------------------------------------ |
| Availability    | Is the system responding?                  | System returns HTTP 200              |
| Reliability     | Is the response correct?                   | The balance returned is accurate     |
| Fault Tolerance | Does it keep working when components fail? | Works with one database replica down |
| Durability      | Is data preserved despite failures?        | Data survives disk failure           |

â¤ A payment system that charges customers twice is available (it processes requests) but unreliable (it processes them incorrectly).
â¤ A database that loses writes during failover is fault-tolerant (it continues operating) but not durable (data was lost).

âš¡Note: These properties are related but independent. You can optimize for one while accidentally sacrificing another.
ğŸ‘‰ A system that aggressively caches data for availability might serve stale (incorrect) responses, trading reliability for uptime.


ğŸ”„ Measuring Reliability
You cannot improve what you do not measure. Reliability has several well-established metrics that help you understand how your system behaves over time.

1ï¸âƒ£ Mean Time Between Failures (MTBF)
MTBF measures the average time between failures. A higher MTBF means failures are less frequent.

MTBF = Total Operating Time / Number of Failures

âœ… Example:
  - System ran for 10,000 hours
  - Experienced 5 failures
  - MTBF = 10,000 / 5 = 2,000 hours

â¤ An MTBF of 2,000 hours means you can expect one failure roughly every 83 days. This helps with planning.
â¤ If you have 100 servers each with MTBF of 2,000 hours, you will see approximately one server failure per day across your fleet.


2ï¸âƒ£ Mean Time To Recovery (MTTR)
MTTR measures how long it takes to restore the system after a failure. A lower MTTR means faster recovery.

MTTR = Total Downtime / Number of Failures
âœ… Example:
  - 5 failures occurred
  - Total repair time: 10 hours
  - MTTR = 10 / 5 = 2 hours per failure

â¤ MTTR includes detection time, diagnosis time, repair time, and verification time.
â¤ Reducing MTTR often has more impact than reducing failure rate. If you cannot prevent failures, at least recover quickly.


3ï¸âƒ£ Error Rate
Percentage of requests that result in errors.

Error Rate = Failed Requests / Total Requests Ã— 100%

| System Type      | Target Error Rate | Meaning                |
| -------------------- | --------------------- | -------------------------- |
| Critical systems | < 0.01%               | 1 in 10,000 requests fails |
| Standard systems | < 0.1%                | 1 in 1,000 requests fails  |
| Tolerant systems | < 1%                  | 1 in 100 requests fails    |


4ï¸âƒ£ Data Correctness
Percentage of responses that contain correct data.

Correctness = Correct Responses / Total Responses Ã— 100%

â¤ This is the often-overlooked metric. A system can have 99.99% availability and 0.01% error rate, but if 1% of successful responses contain wrong data, you have a reliability problem.
â¤ Users received a response, it just was not the right one.


â“ Why Systems Become Unreliable
Reliability failures come from multiple sources.

1ï¸âƒ£ Hardware Failures
â¤ Physical components wear out and fail.
â¤ Disks develop bad sectors. Memory corrupts. CPUs overheat. Network cards malfunction.

â¤ The good news is that hardware failures are well-understood and follow predictable statistical patterns.
â¤ The bad news is that with enough hardware, failures become a daily occurrence.


2ï¸âƒ£ Software Bugs
â¤ Software bugs are the leading cause of reliability problems. Unlike hardware failures, which tend to be random and independent, software bugs are systematic. Every request that hits the buggy code path fails.
â¤ The most dangerous bugs are those that do not crash the system but silently produce wrong results.


3ï¸âƒ£ Configuration Errors
â¤ Configuration mistakes are surprisingly common and often catastrophic.
â¤ A typo in a configuration file can bring down an entire service.


4ï¸âƒ£ Human Error
â¤ Studies show that human error is the leading cause of outages.
â¤ Operators mistype commands. Engineers deploy untested changes. On-call responders misdiagnose problems.


5ï¸âƒ£ Overload and Cascading Failures
â¤ Systems that work perfectly under normal load can fail catastrophically when overloaded.
â¤ When one component slows down, requests queue up, timeouts fire, retries multiply the load, and the problem cascades.


ğŸ”„ Key Principles of Reliable Systems
To build reliable systems, engineers typically focus on several core principles:

1ï¸âƒ£ Redundancy
â¤ Redundancy means having backup components ready to take over if one part fails.
â¤ This could involve multiple servers, duplicate network paths, or backup databases.


2ï¸âƒ£ Failover Mechanisms
â¤ Failover is the process by which a system automatically switches to a redundant or standby component when a failure is detected.
â¤ This ensures continuous operation without noticeable disruption to users.


3ï¸âƒ£ Load Balancing
â¤ Load balancing distributes incoming traffic across multiple servers.
â¤ This not only improves performance but also prevents any single server from becoming a single point of failure.


4ï¸âƒ£ Monitoring and Alerting
â¤ A reliable system is constantly monitored.
â¤ Tools and dashboards track system health and performance, while alerting mechanisms notify engineers of issues before they escalate into major problems.


5ï¸âƒ£ Graceful Degradation
Even when parts of the system fail, a well-designed system can still provide core functionality rather than going completely offline. This concept is known as graceful degradation.


ğŸ”„ Techniques to Enhance Reliability
Now that we understand the principles, letâ€™s look at some practical techniques to implement reliability in your systems.

1ï¸âƒ£ Redundant Architectures
The most fundamental reliability technique is having more components than you need. If one fails, others continue operating.

                +---------+
                |  Users  |
                +---------+
                     |
                     v
              +----------------+
              | Load Balancer  |
              +----------------+
                 |      |      |
        +-----------+ +-----------+ +-----------+
        | Server A  | | Server B  | | Server C  |
        |  ACTIVE   | |  ACTIVE   | |  ACTIVE   |
        +-----------+ +-----------+ +-----------+


âœ… For example: if you have a web server handling user requests, deploy several servers behind a load balancer:
â¤ If one server fails, the load balancer automatically routes traffic to the remaining servers.


2ï¸âƒ£ Data Replication
Ensure your data is not stored in a single location. Use data replication strategies across multiple databases or data centers.

                +----------------+
                |  Application   |
                +----------------+
                         |
                         v
                +----------------+
                |   Primary DB   |
                | (Read/Write)   |
                +----------------+
                     |
          -----------------------------
          |                           |
          v                           v
   +---------------+         +---------------+
   | Replica DB 1  |         | Replica DB 2  |
   |  (Read Only)  |         |  (Read Only)  |
   +---------------+         +---------------+
ğŸ‘‰ This way, if one database fails, the system can still access a copy from another location.


3ï¸âƒ£ Graceful Degradation
â¤ When parts of the system fail, graceful degradation keeps the core functionality working.
â¤ Instead of complete failure, the system provides reduced service.

                     +----------------------+
                     |   Full Service       |
                     | All features         |
                     | available            |
                     +----------------------+
                               |
                         Load increases
                               |
                               v
                     +----------------------+
                     |   Partial Service    |
                     | Non-critical         |
                     | features disabled    |
                     +----------------------+
                               |
                       Components fail
                               |
                               v
                     +----------------------+
                     |     Core Only        |
                     | Only essential       |
                     | functions            |
                     +----------------------+
                               |
                        Critical failure
                               |
                               v
                     +----------------------+
                     |   Emergency Mode     |
                     | Minimal              |
                     | functionality        |
                     +----------------------+

Consider an e-commerce site:
1ï¸âƒ£ Full service â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Personalized recommendations, real-time inventory, all payment options
2ï¸âƒ£ Partial service â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Generic recommendations, cached inventory, primary payment options
3ï¸âƒ£ Core only â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Browse products, checkout with basic payment
4ï¸âƒ£ Emergency mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Display cached product pages, accept orders for later processing


4ï¸âƒ£ Circuit Breakers
â¤ In a microservices architecture, one service failing can cascade failures throughout the system.
â¤ Circuit breakers detect when a service is failing and temporarily cut off requests to prevent overload, allowing the system to recover gracefully.

                +----------------------+
                |        Start         |
                +----------------------+
                           |
                           v
                +----------------------+
                |        Closed        |
                | Normal operation    |
                | Requests pass       |
                | through             |
                +----------------------+
                           |
           Failures exceed threshold
                           |
                           v
                +----------------------+
                |         Open         |
                |     Failing fast     |
                | Requests rejected   |
                | immediately         |
                +----------------------+
                           |
                    Timeout expires
                           |
                           v
                +----------------------+
                |       Half-Open      |
                |   Testing recovery  |
                | Limited requests    |
                | allowed             |
                +----------------------+
                    |             |
      Test request succeeds   Test request fails
                    |             |
                    v             v
        +------------------+   +----------------------+
        |      Closed      |   |         Open         |
        | Normal operation |   |   Failing fast       |
        | Requests pass    |   | Requests rejected    |
        | through          |   | immediately          |
        +------------------+   +----------------------+

â¤ In the closed state, requests pass through normally. Failures are counted. When failures exceed a threshold (e.g., 5 failures in 30 seconds), the circuit opens.

â¤ In the open state, requests fail immediately without calling the dependency. This prevents wasting resources and allows the dependency time to recover.

â¤ After a timeout (e.g., 30 seconds), the circuit moves to half-open. A limited number of test requests are allowed through. If they succeed, the circuit closes. If they fail, it opens again.


5ï¸âƒ£ Idempotency
Network failures make it unclear whether a request succeeded or failed. If you retry, you might execute the operation twice. Idempotent operations produce the same result regardless of how many times they are executed.

WITHOUT IDEMPOTENCY                          WITH IDEMPOTENCY
--------------------                         --------------------

Client                                      Client
  |                                           |
  | Transfer $100 (ID: abc123)                | Transfer $100 (ID: abc123)
  v                                           v
Server                                      Server
  |                                           |
  | Insert transfer abc123                    | Insert transfer abc123
  v                                           v
Database                                    Database
  |                                           |
  | (Row inserted)                            | (Row inserted with ID)
  |                                           |
  | Response lost âŒ                          | Response lost âŒ
  |                                           |
Client retries                               Client retries
  |                                           |
  | Transfer $100 (ID: abc123) [retry]        | Transfer $100 (ID: abc123) [retry]
  v                                           v
Server                                      Server
  |                                           |
  | Insert transfer abc123 âŒ                 | Insert transfer abc123
  |                                           | â†’ Already exists, ignore
  v                                           v
Database                                    Database
  |                                           |
  | $100 transferred AGAIN                    | Only $100 transferred
  | ($200 total âŒ)                           | ($200 avoided âœ…)

â¤ The idempotency key (abc123) allows the server to detect retries.
â¤ The first execution stores the key. Subsequent executions with the same key return the stored result without re-executing.

â¤ Stripe, PayPal, and other payment processors require idempotency keys for money-moving operations.
â¤ Without them, network issues could cause duplicate charges.


============================
ğŸ”· Single Point of Failure
============================
A Single Point of Failure (SPOF) is a component in your system whose failure can bring down the entire system, causing downtime, potential data loss, and unhappy users.

                +---------+
                | Clients |
                +---------+
                     |
                     v
              +----------------+
              | Load Balancer  |
              |  (SPOF) âŒ     |
              +----------------+
                 |      |      |
        +-----------+ +-----------+ +-----------+
        | Server 1  | | Server 2  | | Server 3  |
        +-----------+ +-----------+ +-----------+

              âŒ If Load Balancer fails
              âŒ Clients cannot reach servers

â¤ In the above example, if there is only one instance of the load balancer, it becomes a SPOF.
â¤ If it goes down, clients wonâ€™t be able to communicate with the servers.
â¤ By minimizing the number of SPOFs, you can improve the overall reliability and availability of the system.


1ï¸âƒ£ Understanding SPOFs
A Single Point of Failure (SPOF) is any component within a system whose failure would cause the entire system to stop functioning.

âœ… For Example â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Imagine a bridge that connects two cities. If it's the only route between them and it collapses, the cities are cut off. In this scenario, the bridge is the single point of failure.

â¤ In distributed systems, failures are inevitable. Common causes include hardware malfunctions, software bugs, power outages, network disruptions, and human error.
â¤ While failures can't be entirely avoided, the goal is to ensure they donâ€™t bring down the entire system.
â¤ In system design, SPOFs can include a single server, network link, database, or any component that lacks redundancy or backup.

Letâ€™s see an example of a system and various single points of failures in it:

+-----------+          +-----------+
| Client 1  |          | Client 2  |
+-----------+          +-----------+
        \                   /
         \                 /
          v               v
              +----------------+
              | Load Balancer  |
              +----------------+
                   |      |
                   |      |
           +---------------+   +---------------+
           |  App Server 1 |   |  App Server 2 |
           +---------------+   +---------------+
                   |      |
                   |      |
           +------------------+
           |   Redis Cache    |
           +------------------+
                   |
                   v
              +------------+
              |  Database  |
              +------------+

This system has one load balancer, two application servers, one database, and one cache server.

â¤ Clients send requests to the load balancer, which distributes traffic across the two application servers.
â¤ The application servers retrieve data from the cache if it's available, or from the database if it's not.

In this design, the potential SPOFs are:
1ï¸âƒ£ Load Balancer â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ If there is only one load balancer instance and it fails, all traffic will stop, preventing clients from reaching the application servers. To avoid this, we can add a standby load balancer that can takeover if the primary one fails.

2ï¸âƒ£ Database â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ With only one database, its failure would result in data being unavailable, causing downtime and potential data loss. We can avoid this by replicating the data across multiple servers and locations.

3ï¸âƒ£ Cache Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ The cache server is not a true SPOF in the sense that it doesnâ€™t bring the entire system down. When itâ€™s down, every request hits the database, increasing its load and slowing response times.
The application servers are not SPOFs since you have two of them. If one fails, the other can still handle requests, assuming the load balancer can distribute traffic effectively.


â“ How to Identify SPOFs in a Distributed System
1ï¸âƒ£  Map Out the Architecture
â¤ Create a detailed diagram of your system's architecture. Identify all components, services, and their dependencies.
â¤ Look for components that do not have backups or redundancy.


2ï¸âƒ£ Dependency Analysis
â¤ Analyze dependencies between different services and components.
â¤ If a single component is required by multiple services and does not have a backup, it is likely a SPOF.


3ï¸âƒ£ Failure Impact Assessment
â¤ Assess the impact of failure for each component.
â¤ Perform a "what if" analysis for each component.
â¤ Ask questions like, â€œWhat if this component fails?â€ If the answer is that the system would stop functioning or degrade significantly, then that component is a SPOF.


4ï¸âƒ£ Chaos Testing
â¤Chaos testing, also known as Chaos Engineering, is the practice of intentionally injecting failures and disruptions into a system to understand how it behaves under stress and to ensure it can recover gracefully.

â¤Chaos engineering often involves the use of tools like Chaos Monkey (developed by Netflix) that randomly shut down instances or services to observe how the rest of the system responds.

â¤This can help us identify components that, if they fail, would cause a significant impact on the system.
 


ğŸ”„ Strategies to Avoid Single Points of Failures
1ï¸âƒ£ Redundancy
â¤ The most common way to avoid SPOFs is by adding redundancy. Redundancy means having multiple components that can take over if one fails.
â¤ Redundant components can be either active or passive. Active components are always running.
â¤ Passive (standby) components are only used as a backup when the active component fails.


2ï¸âƒ£ Load Balancing
â¤ Load balancers distribute incoming traffic across multiple servers, ensuring no single server becomes overwhelmed.
â¤ They help avoid single point of failures by detecting failed servers and rerouting traffic to healthy instances.
â¤ To prevent the single load balancer becoming a single point of failure, we can add a standby load balancer which can take over if the primary one fails.


3ï¸âƒ£ Data Replication
Data replication involves copying data from one location to another to ensure that data is available even if one location fails.

1ï¸âƒ£ Synchronous Replication â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Data is replicated in real-time to ensure consistency across locations.
2ï¸âƒ£ Asynchronous Replication â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Data is replicated with a delay, 
which can be more efficient but may result in slight data inconsistencies.


4ï¸âƒ£ Geographic Distribution
Distributing services and data across multiple geographic locations mitigates the risk of regional failures.

This includes using:
1ï¸âƒ£ Content Delivery Networks (CDNs) to distribute content globally, improving availability and reducing latency.
2ï¸âƒ£ Multi-Region Cloud Deployments to ensure that an outage in one region does not disrupt your entire application.


5ï¸âƒ£ Graceful Handling of Failures
Design applications to handle failures without crashing.

âœ… Example â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ If a service that provides user recommendations fails, the application should still function, perhaps with a message indicating limited features temporarily.

ğŸ‘‰ Implement failover mechanisms to automatically switch to backup systems when failures are detected.


6ï¸âƒ£ Monitoring and Alerting
Proactive monitoring helps detect failures before they lead to major outages.

Key practices include:
1ï¸âƒ£ Health Checks â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Automated tools that perform regular health checks on components.
2ï¸âƒ£ Automated Alerts â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Alerts and notifications sent when a component fails or behaves abnormally.
3ï¸âƒ£ Self-Healing Systems â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Systems that automatically recover from failures, such as auto-scaling to replace failed servers.


=======================================
ğŸ”· Latency vs Throughput vs Bandwidth
========================================
Understanding these metrics is crucial for:
1ï¸âƒ£ Diagnosing performance bottlenecks
2ï¸âƒ£ Making informed architectural decisions
3ï¸âƒ£ Setting realistic expectations with stakeholders
4ï¸âƒ£ Answering system design interview questions


ğŸ”„ The Highway Analogy
Before diving into definitions, let us use a simple analogy. Think of a highway connecting two cities:

                   City A
                     |
                     v
        =================================================
        ||  Lane 1  ||  Lane 2  ||  Lane 3  ||  Lane 4 ||
        || (Car)    || (Car)    || (Car)    || (Car)   ||
        =================================================
                     ğŸš§ Accident / Traffic Jam
                     â†“ (Only few cars pass)
        -------------------------------------------------
                     |
                     v
                   City B


1ï¸âƒ£ Bandwidth is the number of lanes on the highway. More lanes mean more cars can travel simultaneously.
2ï¸âƒ£ Throughput is how many cars actually pass through per hour. This depends on traffic conditions, not just the number of lanes.
3ï¸âƒ£ Latency is the time it takes for a single car to travel from one city to the other.

A highway might have 4 lanes (high bandwidth), but if there is an accident, only 100 cars per hour pass through (low throughput). Meanwhile, each car might take 2 hours to complete the journey (high latency).

This analogy helps explain why these metrics do not always move together.


1ï¸âƒ£ Latency
Latency is the time it takes for a single request to travel from source to destination and back. It measures delay.

In networking, latency is often called round-trip time (RTT), the time from sending a request to receiving a response.

ğŸ”„ Round-Trip Time Block Diagram
Client                                   Server
  |                                        |
  |---- Request (t = 0 ms) --------------->|
  |                                        |
  |            Processing (50 ms)          |
  |                                        |
  |<--- Response (t = 150 ms) --------------|
  |                                        |


â±ï¸ Latency Breakdown
Client â†’ Server   : 50 ms  (Network delay)
Server Processing : 50 ms
Server â†’ Client   : 50 ms  (Network delay)
------------------------------------------
Total Latency     : 150 ms (Round-Trip Time)


Components of Latency
Latency is not a single value. It is the sum of multiple delays:

Client â†’ Network Latency â†’ Server Processing â†’ Database Query â†’ Response Assembly â†’ Network Latency â†’ Client


1ï¸âƒ£ Propagation delay â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time for signals to travel through the medium. Light in fiber travels at ~200,000 km/s. A cross-Atlantic request (6,000 km) takes ~30ms just for propagation.
2ï¸âƒ£ Transmission delay â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time to push bits onto the wire. Depends on packet size and link bandwidth.
2ï¸âƒ£ Processing delay â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time for routers, load balancers, and servers to process packets.
4ï¸âƒ£ Queuing delay â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time spent waiting in buffers when components are busy.


ğŸ”„ Measuring Latency
Latency is typically measured using percentiles:
| Metric       | Description                                  |
| -------------| -------------------------------------------- |
| p50 (Median) | 50% of requests are faster than this value   |
| p95          | 95% of requests are faster than this value   |
| p99          | 99% of requests are faster than this value   |
| p99.9        | 99.9% of requests are faster than this value |

â“ Why percentiles matter: Average latency hides outliers. A system with 10ms average might have p99 of 500ms, meaning 1% of users experience terrible performance.

What Affects Latency?
| Factor              | Impact                                                 |
| ----------------------- | ---------------------------------------------------------- |
| Geographic distance | More distance = higher propagation delay                   |
| Network congestion  | Causes queuing delays and packet retransmissions           |
| Server load         | High load increases request processing time                |
| Database queries    | Slow or unoptimized queries add significant latency        |
| DNS resolution      | Cold requests require DNS lookup, adding delay             |
| TLS handshake       | Adds 1â€“2 extra network round trips during connection setup |


ğŸ”„ Reducing Latency
1ï¸âƒ£ Use CDNs â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Serve content from edge locations closer to users
2ï¸âƒ£ Caching â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Eliminate round trips by caching at multiple layers
3ï¸âƒ£ Connection pooling â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Avoid repeated connection setup
4ï¸âƒ£ Database optimization â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Add indexes, optimize queries
5ï¸âƒ£ Geographic distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Deploy servers closer to users
6ï¸âƒ£ Protocol optimization â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Use HTTP/2, HTTP/3 (QUIC)


2ï¸âƒ£ Throughput
Throughput is the amount of work completed per unit of time. It measures volume.

For web systems, throughput is often expressed as requests per second (RPS) or transactions per second (TPS).

Incoming Requests
   |
   |   Req 1   Req 2   Req 3   Req 4   Req 5
   v
+---------------------------------------------+
|                 Server                      |
|             Processing Capacity             |
|                1000 RPS                     |
+---------------------------------------------+
   |
   |   Resp 1  Resp 2  Resp 3  Resp 4  Resp 5
   v
Completed Responses


ğŸ”„ Throughput vs Bandwidth
A common confusion â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ bandwidth is theoretical maximum capacity, while throughput is actual achieved rate.
| Metric     | Definition                                        | Example              |
| -------------- | ----------------------------------------------------- | ------------------------ |
| Bandwidth  | Maximum possible data transfer rate of a network      | 1 Gbps network link      |
| Throughput | Actual data transfer rate achieved in real conditions | 600 Mbps actual transfer |


You can never have throughput higher than bandwidth, but throughput is almost always lower due to:
1ï¸âƒ£ Protocol overhead (headers, acknowledgments)
2ï¸âƒ£ Congestion and packet loss
3ï¸âƒ£ Processing limitations
4ï¸âƒ£ Inefficient resource utilization


ğŸ”„ Calculating Throughput
1ï¸âƒ£ For a single-threaded system:
Throughput = 1 / Latency

If latency = 10ms:
Throughput = 1 / 0.01s = 100 requests/second


2ï¸âƒ£ For a multi-threaded system:
Throughput = Concurrent Workers / Latency

If latency = 10ms and 50 workers:
Throughput = 50 / 0.01s = 5,000 requests/second


â“ What Limits Throughput?
                         +----------------------+
                         |   Throughput Limiter |
                         +----------------------+
                                     |
        ----------------------------------------------------------------
        |              |                |               |             |
        v              v                v               v             v
+---------------+ +---------------+ +---------------+ +---------------+ +---------------+
|      CPU      | |    Memory     | |      I/O      | |  Connections  | |  Contention   |
| Compute-bound | | Large working | | Disk / Network| | Connection    | | Locks, shared |
| tasks         | | sets          | |               | | limits        | | resources     |
+---------------+ +---------------+ +---------------+ +---------------+ +---------------+

The bottleneck determines maximum throughput. A system is only as fast as its slowest component.

ğŸ”„ Improving Throughput
1ï¸âƒ£ Horizontal scaling â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Add more servers
2ï¸âƒ£ Vertical scaling â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Add more CPU, memory
3ï¸âƒ£ Async processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Do not block on slow operations
4ï¸âƒ£ Batching â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Process multiple items together
5ï¸âƒ£ Caching â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Reduce work by reusing results
6ï¸âƒ£ Connection pooling â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Reuse expensive connections
7ï¸âƒ£ Load balancing â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Distribute work evenly


===============
ğŸ”· Bandwidth
===============
â¤ Bandwidth is the maximum rate at which data can be transferred. It measures capacity.
â¤ Bandwidth is typically expressed in bits per second (bps): Kbps, Mbps, 

+------------------+        High-Capacity Link        +------------------+
|     Sender       | ===============================> |    Receiver      |
| (Client / App)   |        (Bandwidth)              | (Server / App)   |
+------------------+   e.g., 1 Gbps / 10 Gbps         +------------------+


ğŸ”„ Types of Bandwidth
| Type              | Description                   | Example        |
| --------------------- | --------------------------------- | ------------------ |
| Network bandwidth | Capacity of network links         | 1 Gbps Ethernet    |
| Memory bandwidth  | Rate of data transfer to/from RAM | DDR4 â‰ˆ 25 GB/s     |
| Disk bandwidth    | Read/write speed of storage       | SSD â‰ˆ 500 MB/s     |
| Bus bandwidth     | Internal data transfer rate       | PCIe 4.0 â‰ˆ 64 GB/s |


ğŸ”„ Bandwidth-Delay Product
An important concept that connects bandwidth and latency:
Bandwidth-Delay Product (BDP) = Bandwidth Ã— Latency

BDP represents how much data can be "in flight" at any moment.
âœ… Example:
Bandwidth: 1 Gbps = 125 MB/s
Latency: 100ms (coast-to-coast US)
BDP: 125 MB/s Ã— 0.1s = 12.5 MB

ğŸ‘‰ This means 12.5 MB of data can be traveling through the pipe at any instant.
ğŸ‘‰ If your TCP window size is smaller than BDP, you will not fully utilize available bandwidth.


